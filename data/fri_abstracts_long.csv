Day,Time,Title,Speaker,Abstract,Session,Room,Chair
Fri,09:15,Tools for Model-Based Clustering in R,Bettina Grün,"Model-based clustering aims at partitioning observations into groups based on either finite or infinite mixture models. The mixture models used differ with respect to their clustering kernel, i.e., the statistical model used for each of the groups. The choice of a suitable clustering kernel allows to adapt the model to the available data structure as well as clustering purpose. We first give an overview on available estimation and inference methods for mixture models as well as their implementations available in R and highlight common aspects regardless of the clustering kernel.
Then, the design of the R package flexmix is discussed pointing out how it provides a common infrastructure for fitting different finite mixture models with the EM algorithm. The package thus allows for rapid prototyping and the quick assessment of different model specifications. However, only a specific estimation method and models which share specific characteristics are covered. We thus conclude by highlighting the need for general infrastructure packages to allow for joint use of different estimation and inference methods and post-processing tools.",Keynote,Concorde 1+2,Andrea Rau
Fri,10:25,Native Chrome Automation using R,Christophe DervieuxRomain Lesur,"The Chrome web browser can be fully controlled in headless mode. A headless browser is a convenient tool for web scraping and automated testing of web sites: one can program all the actions performed in a web browser thanks to the chrome devtools protocol. It is also useful for creating a PDF or taking a screenshot of a web page. Headless Chrome automation is widely used in the node.js ecosystem mainly through the Puppeteer library.
In this talk, we will show you how to take the full control of Chrome directly from R without any dependency on another language or tool (node.js or Java are not required). We will be looking at the principles that make it possible and at some working examples like the chromeprint() function in pagedown package. We will also illustrate what is possible with some examples using the in-development crrri package (https://github.com/RLesur/crrri) that offers a low level access to the chrome devtools protocol.",Shiny & web,Concorde 1+2,Nic Crane
Fri,10:30,Our journey with Shiny : some packages to enhance your applications,Victor PerrierFanny Meyer,"Shiny has become a must in the R environment. In any sector of activity, {shiny} is no longer only used for prototypes but also for applications in production, to create reports, dashboards, tools... Shiny offers a basic foundation for application development and has a very rich API that allows developers to write their own extensions. During our journey with shiny we were able to write several packages to include new features and take advantage of its hidden gems. We will therefore present you our ecosystem of packages around Shiny: - Make the interface more attractive with {shinyWidgets} - Build Proxy for htmlwidgets for smooth integration into Shiny, an example with {billboarder} - Minimal busy indicator with {shinybusy} - Monitor app usage with {shinylogs}",Shiny & web,Concorde 1+2,Nic Crane
Fri,10:35,auth0: Secure Authentication in Shiny with Auth0,Julio Trecenti,"auth0 is a freemium service used to add authentication in web applications. We present the auth0 R package, which implements solutions to use auth0 with Shiny apps. With a simple interface, it is possible to add authentication interfaces to shiny apps, including social (Google / Facebook / Twitter / Linkedin), enterprise (Active Directory / LDAP), app creator's own database and many others. The auth0 R package needs only two things to work: create a configuration YML file with auth0's API keys, and replace the shiny::shinyApp() function with auth0::shinyAuth0App(). The package also includes i) helpers to collect and operate the logged user information inside the app and ii) logout buttons. The auth0 service can be used offline (localhost), in a shiny-server or even with RStudio's shinyapps.io service. In this lightning talk, we will show how auth0 R package works and, if possible, a live demonstration of an app running with auth0.",Shiny & web,Concorde 1+2,Nic Crane
Fri,10:40,Packaging shiny applications,Maxim Nazarov,"Shiny applications are ubiquitous nowadays and traditionally created as a collection of R script files (optionally with external resources, such as images, css files etc). When the applications grow larger it may become difficult to manage these files.
I will advocate the use of R packages for creation of shiny applications. This approach allows to leverage all the advantages of the R packaging ecosystem, including managing namespaces and dependencies, versioning, documentation and tests. These are especially important when deploying and supporting shiny applications in production. In particular I will talk about using functions to define both ""server"" and ""ui"" components, and separating logical application pieces into shiny modules for easier structuring and sharing.
The aim of this talk is to encourage people to try this approach by discussing the differences with the traditional (file-based) approach, advantages of having an R package and things to watch out for. Finally I will mention how a packaged shiny application can be deployed in an enterprise context using ShinyProxy.",Shiny & web,Concorde 1+2,Nic Crane
Fri,10:45,Photon : Building an electron-shiny app using a simple RStudio add in.,Abbas Rizvi,"Have you ever wanted to deliver your model and Shiny application to someone that can't allow their data to leave their computer, or to deliver your Shiny application as a quick prototype that a user could use as a simple Desktop application without deploying over the Internet? Last year at useR! 2018 Katie Sasso showed how to do so by building an Electron-Shiny executable, which allows you to deliver your Shiny app as a Desktop application (https://www.youtube.com/watch?v=ARrbbviGvjc).  This year Abbas Rizvi is going to take it a step further and show a super easy was to use a newly available RStudio Add-In that makes the process super simple and convenient. RStudio Add-Ins allow the IDE to be extended and customized, creating automated and faster work streams. (https://www.rstudio.com/resources/webinars/understanding-add-ins/). After this talk you will be able to create a Windows or Mac Desktop Shiny App with a few simple clicks of the RStudio Add-in.",Shiny & web,Concorde 1+2,Nic Crane
Fri,10:50,Visualizing Huge Amounts of Fleet Data using Shiny and Leaflet,Andreas Wittmann,"When using huge amounts of fleet data data scientists like me often use Shiny in combination with Leaflet to further investigate such data and furthermore to present the user a first prototype of a future data product. When using Leaflet, you can quickly reach limits on the amount of data. As a rule of thumb, a maximum of 10,000 data points for visualization seem possible here. The solution is often clustering, but sometimes you want to be able to see all the data. Therefore a tile layer based approach is used for the visualization. With this technique, there is virtually no limit to the amount of data. In this Talk I will show how R can be used with the packages Leaflet and Plumber to accomplish this. As an example record, millions of taxi pickups in NYC are used. The data is here persisted in a NoSQL database like Cassandra.",Shiny & web,Concorde 1+2,Nic Crane
Fri,10:25,fxtract - Feature Extraction from Grouped Data,Quay Au,"Researchers and practitioners nowadays have access to large longitudinal datasets. These datasets often need to be aggregated by groups in order to be utilized in statistical analyses or machine learning models, and to enable better understanding by users. Especially for large datasets, this process can be difficult and is often prone to errors.  We present the package ""fxtract"", which provides an easy to use API to facilitate the extraction of user-defined features from grouped data. Similar to the summarize-functionality of the dplyr package, our package helps reducing multiple rows of data into one row for each group. fxtract offers easy manageability of large datasets, since data for each group is only read into memory when needed. The package is written in R6 and parallelization is supported by using the R-Package future.   Project page: https://github.com/QuayAu/fxtract",Methods & applications,Cassiopée,Robin Lovelace
Fri,10:30,Spatial Optimisation with OSRM and R,Megan Beckett,"Open Source Routing Machine (OSRM) is a high-performance routing engine for calculating the shortest paths through a road network. These calculations are available via Google Maps. However, queries against a local OSRM server are orders of magnitude faster than Google Maps. The osrm package for R exposes these routing data to a wide range of potential applications. In this talk I'll show how to easily spin up and provision an OSRM server and use it to solve some interesting spatial optimisation problems in R.",Methods & applications,Cassiopée,Robin Lovelace
Fri,10:35,Anomaly detection in trivago,Peter Brejcak,"Internet business brings huge amount of data and daily challenges. For us at trivago it is very important to correctly detect nonhuman traffic or detect anomalies. We are going to share our real example of an obvious anomaly with business impact: 
 - How was the anomaly detected? 
 - What caused the anomaly? Are we able to answer this confidently? 
 - What action should we take? 
 - How can, and can't R help us?",Methods & applications,Cassiopée,Robin Lovelace
Fri,10:40,Using R and the Tidyverse to Play Fantasy Baseball,Angeline Protacio,"Of the existing R packages that help analyze baseball statistics, few are geared toward fantasy baseball, which benefits from projection data for drafting a team, and updated data for daily analysis to set game day rosters and maintain a fantasy team. This talk describes how to use the tidyverse package suite to bring in baseball projected and actualized data, analyze baseball statistics relevant to fantasy baseball, draft a competitive fantasy baseball team, and maintain this team throughout the course of a season. By applying the tidyverse package suite of data cleaning and visualization tools to baseball statistical analysis, R users already familiar with this popular suite of packages can learn how to play and succeed at fantasy baseball. Users not already familiar with fantasy sports can expect a quick primer on fantasy baseball and statistics, sourcing baseball data for analysis, and analytic approaches to apply to a league of their own. ",Methods & applications,Cassiopée,Robin Lovelace
Fri,10:45,Optimizing children sleeping time using regression and machine learning,Alicja Fras,"Sleeping child is an everyday dream come true of every young parent. Unwilling to fall asleep child can change one's life into a nightmare and those, who calmly sleep long hours used to be the object of desire. There are many functions, that the parents try to optimize - some simply want their children to sleep as much as possible, some want to wake up later and enjoy their kids in the evenings or the opposite (you cannot have it all), in our case we also wanted to synchronize sleeping times of our two children. To reach the goal, they try to find some patterns in children’s behavior – they impose some daily routines, do not let children nap too long during the daytime or try to put them in bed earlier, hoping that slower pace will lull them. However intuitions may be misleading, and that is why after a few months of observing my children sleeping patterns I decided to hire statistical tools. I wanted to verify the hypothesis if we should wake them up earlier in the morning (which in case of our kids turned out to be true), but the framework allowed me to detect also some more useful patterns and draw hints. After first attempts with linear regression, I decided to introduce my kids to the neural network and let it learn. My further idea is to create a web app, where one can upload a data file with observed sleeping times and habits, which would help draw reliable conclusions. ",Methods & applications,Cassiopée,Robin Lovelace
Fri,10:25,Adjusting reviewer scores for a fairer assessment via multi-faceted Rasch modelling,Caterina Constantinescu,"Selecting submissions for a conference can be viewed as a measurement problem: in principle, organisers aim to accept the ""best"" submissions, but the precise manner in which this is achieved can vary considerably. It is also common to involve multiple reviewers in the process, and it may not always be the case that all reviewers manage to rate all submissions. Hence, there is a chance that some particularly harsh reviewers may rate the same submission (and put it at a disadvantage), or some more lenient reviewers may happen to rate the same submission and propel it higher in the ranking. A solution to this issue is offered by multi-faceted Rasch models, which view submission scores as a function of not just the quality of the submission in itself, but also reviewer severity. This allows to adjust submission scores accordingly, providing a fairer measurement process. Conveniently, the R package `TAM` allows to estimate this type of model. In this talk, I will walk you through an example of how `TAM` was used on data collected as part of the review process for a data science conference in Scotland.",Models & methods,Caravelle 2,Michela Battauz
Fri,10:30,Penalized regressions to study multivariate linear models : the VariSel package.,Marie Perrot-Dockès,"We propose a new methodology to study multivariate linear models. It consists in estimating the covariance matrix of the responses beforehand and plugging this estimator into a penalized regression model to perform variable selection. Depending on the application field and the data at hand, different penalizations are preferred. For example, recoursing on a l1-norm penalty performs a ""simple"" selection in the regression coefficients. In certain situations, we need to select the same variable for all the responses or a group of variables all together. This can be done by means of a penalty similar to the sparse group Lasso (Yuan and Lin (2006)). Another example is the fused-Lasso penalty, where one want to influence variables to have the same coefficient (Hoefling (2010)). We study an example in genomic for study water-use efficiency (WE) of vine. The covariables are the number of replicates of the different alleles of different genes and the responses characterize the WE of the different vines. The aim is to select genes or alleles that have an effect on this WE. We consider several ways to study the problem: either by selecting an allele of a gene for a specific response or by influencing it to have the same coefficients for all the responses. It can also be interesting to select all the alleles of the same gene together. The presentation will show how to do all of this with the VariSel package.",Models & methods,Caravelle 2,Michela Battauz
Fri,10:35,"Maximum spacing estimation, a new method in fitdistrplus",Christophe Dutang,"Maximum spacing estimation (MSE) introduced by [1] is a method for estimating the parameters based on the probability differences of order statistics. More precisely, the method consists in maximizing of the geometric mean of spacings in the data, which are the differences between the values of the distribution function at sorted observations. MSE has been proved to be reliable method both for heavy-tailed models by [2] and lighted-tailed models by [1]. Currently, only the BMT package provides the MSE for a single distribution. In this talk, we study the development and integration of this method in the fitdistrplus package [3]. This latter package provides maximum likelihood and other methods for any distribution characterized by d, p, q functions. Using the S3 class and generic methods, this estimation method nicely fits the package framework. An uncertainty analysis is carried out on simulated datasets from both light and heavy tailed models. Finally, we illustrate the relevancy of this method to model insurance losses on real actuarial datasets.
[1] Ranneby, Bo (1984). The maximum spacing method. An estimation method related to the maximum likelihood method. 
[2] Wong, T.S.T; Li, W.K. (2006). A note on the estimation of extreme value distributions using maximum product of spacings. 
[3] M. L. Delignette-Muller, C. Dutang (2015). fitdistrplus: An R Package for Fitting Distributions.",Models & methods,Caravelle 2,Michela Battauz
Fri,10:40,rama: an R interface to the GAMA agent-based modeling platform,Marc Choisy,"Agent-based models (ABM) are discrete-event computer models focusing on the interactions between autonomous entities. The downsides of a high flexibility are that (i) ABM must be defined algorithmically (requiring programming) and (ii) ABM are black-box models, the study of which resort to large numbers of simulations. The GAMA modeling platform offers a user-friendly language (GAML) and environment to develop and efficiently run ABM, potentially in an explicit spatial environment. It lacks, however, the statistical tools to exploit ABM simulations. The rama package is an R interface to GAMA intended to fill this gap. The package defines the S3 class experiment that inherits from the tbl_df class and that is tidyverse compatible. An experiment object is linked to an ABM model defined in a GAML (text) file and contains, for this model, a number of simulations (in rows) that differ by their seed and/or parameters values (in columns). Methods are available to intuitively subset and combine experiment objects, as well as to generate experimental designs for experiment objects. Among them, run_experiment() seamlessly calls the GAMA engine to run the simulations of an experiment object and returns it augmented with a list-column of data frames of time series of the simulations outputs. Simulations outputs can then be analysed in R for model calibration, sensitivity analysis and hypotheses testing.",Models & methods,Caravelle 2,Michela Battauz
Fri,10:45,RcppGreedySetCover: Scalable Set Cover,Kaeding Matthias,"The set cover problem is of fundamental importance in the field of approximation algorithms: Given a collection of sets, find the smallest sub-collection, so that all elements from a universe are covered. A diverse range of real world problems can be represented as set cover instance such as location-allocation, shift planning or virus detection. An optimal solution to the problem via linear programming is available. Due to the computational costs involved, this is not a feasible solution for large problems. A quick approximation is given by the greedy algorithm. This talk introduces RcppGreeySetCover, an implementation of the greedy set cover algorithm using Rcpp. The implementation is fast due to the reliance on efficient data structures made available by the Boost headers and the C++ 11 standard library. Preprocessing of input data is done efficiently via the data.table package. Input and output data is a tidy two column data.frame.  As a case study, we apply the package on a large scale (>= 100 million rows) hospital placement problem, which initially motivated the creation of the package.",Models & methods,Caravelle 2,Michela Battauz
Fri,10:50,The GPareto and GPGame packages for multi and many objective Bayesian optimization,Mickaël Binois,"The GPareto package provides multi-objective optimization algorithms for expensive black-box functions and an ensemble of dedicated uncertainty quantification methods. Popular methods such as efficient global optimization in the mono-objective case rely on Gaussian processes or Kriging to build surrogate models. Several infill criteria have also been proposed in a multi-objective setup to select new points sequentially and efficiently cope with severely limited evaluation budgets. They are implemented, in addition to Pareto front estimation and uncertainty quantification visualization in the design and objective spaces. Rather than estimating the entire Pareto front, the GPGame package focus on finding equilibrium solutions (e.g., Nash equilibrium) that are better suited for larger number of objectives.",Models & methods,Caravelle 2,Michela Battauz
Fri,10:25,The transition from conventional tools in banking to R,Balazi Peter,"Presenter: Peter Balazi Focus: Historical and future usage of R in credit risk modeling and its application Abstract: The highly regulated commercial banking industry has been slow and in general averse to change. To blame is the system legacy, processes but also managerial unwillingness. Historically and to these days, the commercial banking industry, especially risk management area, has been dominated by large software companies, SAS and alike. However this is changing rapidly, mainly led by the inner initiative of software end users. Now also increasingly recognized by management, of a need to change and adapt to the demand of end-users by realizing its potential and align to market place to attract talent. This presentation will briefly discuss the historical, current and future direction of a transition from typical conventional commercial type of statistical software (SAS) to R. Practical industry examples will be provided/shown where the choice of the statistical language is crucial and/or preferred in achieving the desired output giving the existing constrains. Brief view into this industry and best practice will be provided and encouragement given for R user community that better days lay ahead. Welcome for cooperation and sharing of knowledge will be also offered for those interested more details provided during the break in 1:1 session while approach in person.",Switching to R,Saint-Exupéry,Gergely Daroczi
Fri,10:30,"R++, a new Graphical User Interface for R",Christophe Genolini,"R++ is a new Graphical User Interface for R. It is intended for those who are not statisticians (doctors, psychologists, salespeople,...) but who nevertheless need high-level professional statistics. R++ is a result of collaborations with different computer-science labs specialised in Human-Computer Interaction. Through twenty or so meetings with statisticians, we have identified the tasks that are considered to be arduous, annoying or of low added value. 3 aspects have arisen: reading data (encoding problems, column separators, odd formats, distant data bases), data managment (outliers' detection, modalities' merging,...) and exporting results (text or graph). R ++ offers an easy-to-use interface simplifying processing these tasks. In reading the data, a preview is displayed allowing checking of the data and thus to proceed to various settings. For processing the data, several tools are available. For example, it is possible to instantly graphicaly represent all the variables of a database or to correct the types using a colour system. For exporting the data, interface allows adjustments to graphical settings, then a simple drag&drop can export graphs. Naturally, for the sake of reproducibility of analyses, each action generates the corresponding R code. A code editor and an R console are also available for more advanced uses. R++ is free for academics, students, and associations.",Switching to R,Saint-Exupéry,Gergely Daroczi
Fri,10:35,R in Pharma: A tailored approach to converting programmers to R in an industry resistant to change,Kieran Martin,"The pharmaceutical industry has historically, overwhelmingly made use of one software package, SAS. Almost every submission made to regulatory agencies such as the FDA and the EMA have been made using SAS code, and SAS data formats. For those of us who are enthusiastic about promoting usage of R, this can lead to an uphill struggle, but things are changing. R has become more and more popular, and appetite for R usage is growing. In this talk I will discuss some of the efforts to promote R within Roche, with a focus on two particular methods: - Targeted training, focused on problems programmers have to solve every day - Tidyverse training, and a focus on functional programming - In house data and as close to real examples as possible, to show direct applications for work in R - New R packages. Multiple different packages have been developed to support different activities. In this talk I will focus primarily on diffdf, a package that is available on CRAN, which used allows detailed dataset comparison, meeting an unmet need",Switching to R,Saint-Exupéry,Gergely Daroczi
Fri,10:40,Community Driven Data Science in Insurance,Kevin Kuo,"We introduce Kasa AI, a community driven initiative for open research and software development in insurance and actuarial analytics. Open source software has been credited for recent rapid advances in machine learning and its applications in various industries. The insurance industry, being a heavily regulated industry, has been slower to embrace open source, but recent trends indicate that actuaries are shifting their workflows to adapt to new technologies. We discuss motivations for the community, current projects, which span both life and nonlife insurance, and how tooling in the R ecosystem has enabled reproducible research at scale.",Switching to R,Saint-Exupéry,Gergely Daroczi
Fri,10:45,unconfUROS and one of its outputs vornoiTreemap,Alexander Kowarik,"The annual conference on the use of R in official statistics is since 2018 enriched by a side event similar to the rOpenSci Unconfs. Similar to the idea of the awesome official statistics software list, this should complement an abstract top-down approach such as the common statistical production architecture with fast bottom-up solutions. One of the implemented ideas 2018 was to provide an easy way to generate Voronoi treemaps in R. The motivation came from examples of the ""Price Kaleidocscope"" from the federal statistical office of Germany. The newly released CRAN package voronoiTreemaps provides the functionality to create Voronoi treemaps with the JavaScript library d3-voronoi-treemap and to integrate them into a shiny app.",Switching to R,Saint-Exupéry,Gergely Daroczi
Fri,10:50,An R implementation of a model-based estimator  –  a UK case study,Konstantinos Soulanis,"The UK Office for National Statistics (ONS) uses a model-based conditional ratio estimator to estimate the product by industry sales of products produced by the UK manufacturing sector (the PRODCOM survey) and provided by the UK services sector (the Annual Survey of Goods and Services - ASGS). As ASGS is a recently developed survey by the ONS, R was chosen to be part of the production system over other software packages because of its abilities to handle large datasets (approximately 2000 product by 300 industries by 40000 businesses) and complex calculations efficiently and in a timely manner. R also had its advantages in that the results can be easily visualised as part of the processing. Using the recent set of tidyverse based packages this was able to be done in such a way for the methodology to be implemented concisely. This talk will briefly describe the methodology, the pipeline of functions created and a comparison of the outputs to the design-based Horvitz-Thompson (expansion) estimator. It is the author's hope that this pipeline will be packaged up and be made available for other National Statistics Offices and interested researchers who have surveys which require a similar estimation scheme. ",Switching to R,Saint-Exupéry,Gergely Daroczi
Fri,10:55,Using advanced R packages for the visualisation of clinical data in a cancer hospital setting,Roxane Legaie,"NGS-based assays for cancer testing have been increasingly adopted by clinical laboratories, especially targeted gene panels which are now commonly used to guide diagnostic and therapeutic decisions. While there are many databases for the annotation and curation of individual variants, there is a lack of off-the-shelf software for the mining of those databases.   The Pathology Department at Peter MacCallum Cancer Centre conducts variant curation using in-house software, PathOS. This database contains variant information from thousands of clinical tumour samples, from a variety of NGS assays (mostly gene panels), and a range of tumour and tissue types. Visualising variations across all clinical contexts can be used to validate important findings, improve patient treatments and refine tumour classifications.    In this talk I will show how, by using advanced R packages, such as maftools and oncoplots, we are able to interrogate our database and make sense of variants from thousands of patients across multiple cancer types. I will discuss the need for implementing such solutions into our routine curation QC process, and will describe the challenges around working with data stored in various file formats and implementing solutions into a curation system in production.  ",Switching to R,Saint-Exupéry,Gergely Daroczi
Fri,10:25,rGSAn: a R package dedicated to the gene set analysis using semantic similarity measures.,Aarón Ayllón-BenítezPatricia Thebault,"The revolution in high-throughput sequencing technologies, which enables the acquisition of gigabases of DNA sequences, is leading to great applications for understanding the relationships from the genotype to the phenotype. The accumulation of massive amounts of omics data is then providing relevant biological information thanks to the integration of annotation information coming from sources available in the ""omics"" domains. These knowledge resources are then essential for interpreting the functional activity of genes, even though, managing the large number of annotation terms associated to a gene set level is a difficult task. To address this issue, we introduce rGSAn, a R package dedicated to the Gene Set Annotation. By adopting a strategy combining semantic similarity measures, and data mining approaches, rGSAn is able to perform an unified and synthetic annotation for a gene set of interest.To do so, the best partition of the annotation terms is computed using semantic similarity measures. Then, the most relevant terms are identified using a decision tree algorithm and an heuristic implementation of the general set cover problem. In addition, rGSAn offers new visualization facilities to interactively explore the annotation results according to the hierarchical structure of Gene Ontology.",Bioinformatics & biostatistics,Ariane 1+2,Guillaume Devailly
Fri,10:30,Pathway-VisualiseR: An Interactive Web Application for Visualising Gene Networks,Goknur GinerAlexandra Garnham,"RNA-seq based expression profiling allows us to quantify gene expression changes in pertinent biology and to discover a set of biomarkers that are potentially the origin of the biological phenomenon of interest. Moreover, investigating the collective behaviour of the biomarker genes as a set, utilising databases such as Gene Ontology (GO), KEGG (Kyoto Encyclopedia of Genes and Genomes), STRINGdb (Search Tool for the Retrieval of Interacting proteins database), play an important role in gaining a comprehensive knowledge about the relevant biological inquiry. Here, we built a web-based interactive RShiny tool in order to gain further insights into the alterations within gene sets and the changes in the interactions between them. PathwayVisualiseR implements a fit object, which is the output from a linear model that was fit for each gene for a given series of samples, from the limma R package. PathwayVisualiseR, therefore, enables the user to incorporate the results from gene set tests, such as Camera, Roast and Fry. To summary, visualising gene set interactions with PathwayVisualiseR helps in speeding up the process of comprehending large or complex data sets and unfolds the relationship among the functional pathways and biological mechanisms.",Bioinformatics & biostatistics,Ariane 1+2,Guillaume Devailly
Fri,10:35,Compiling a global database of sapflow measurements with R: Workflow and tools for the SAPFLUXNET database,Víctor Granda,"Understanding the global patterns and drivers of plant transpiration and its physiological control requires compiling and harmonising heterogeneous ecophysiological datasets. The SAPFLUXNET project (http://sapfluxnet.creaf.cat/) has compiled the first global database of transpiration from sap flow measurements, including > 10 million sub-daily records from 2732 plants and 176 species, measured in > 200 globally distributed datasets.  Here we present the SAPFLUXNET data infrastructure, which allows implementing a reproducible workflow in the R environment. The ""sapfluxnetQC1"" package (https://github.com/sapfluxnet/sapfluxnetQC1) is an internal-use package which implements data harmonisation and quality control, using Rmarkdown-generated reports and interactive Shiny apps. The ""sapfluxnetr"" package (https://github.com/sapfluxnet/sapfluxnetr) is aimed to ease data access, aggregation and analysis, using two new S4 classes (""sfn-data"", and ""sfn_data_multi""). The SAPFLUXNET database and its data infrastructure have been designed to be open and publicly available, supporting new data-driven analysis of global vegetation functioning and also facilitating future updating and maintenance.",Bioinformatics & biostatistics,Ariane 1+2,Guillaume Devailly
Fri,10:40,Bayesian sequential integration within a preclinical PK/PD modeling framework using rstan package: Lessons learned,Fabiola La Gamba,"Although Bayesian methods are expanding considerably, their applications in the field of pharmacokinetic/pharmacodynamic (PK/PD) modelling is still relatively limited. In this work, Bayesian techniques are used to estimate a novel PK/PD model developed to quantify the PD synergy between two compounds using historical in vivo data. The model is fitted using package rstan, the R interface to Stan. Stan is a recently developed software package which allows an efficient estimation using the No-U-Turn Sampler (NUTS). rstan facilitates the use of this powerful tool, allowing to run Stan models within R. Since the data consist of a series of trials performed sequentially, a Bayesian sequential integration is considered: the posteriors resulting from the analysis of one trial are used to specify the priors of the next trial. Challenges and opportunities of this technique will be discussed with respect to: (i) prior specification, (ii) random effect choice, (iii) experimental design. In addition, the results from an extensive simulation study assessing the performance of the Bayesian sequential integration for an increasing model complexity are evaluated. The results suggest that the Bayesian sequential integration is promising in certain settings. The specification of informative priors and a suitable experimental design are nevertheless advisable, especially in preclinical studies.",Bioinformatics & biostatistics,Ariane 1+2,Guillaume Devailly
Fri,10:45,VICI: a Shiny app for accurate estimation of Vaccine Induced Cellular Immunogenicity with bivariate modeling,Boris Hejblum,"In vaccine clinical development, it is important to assess the ability of a candidate vaccine to generate immune responses. Cellular immunogenicity is usually measured by Intra-Cellular Staining (ICS) after specific stimulation. The usual method for the analyzing such data is to i) first subtract the response observed in non-stimulated cells from each stimulated sample; ii) then perform an inter- or intra-arm comparison of the percentages of cells producing cytokine(s) of interest. Step i) aims at capturing the antigen-specific response, but the subtraction may induce biased estimates and compromise type 1 error and harm statistical power. We have proposed a bivariate regression model as a new method for an accurate estimation of the vaccine effect from ICS data in vaccine trials. This allows to model a linear relationship between the non-stimulated response and the antigenic responses, while taking into account both of their measurement errors, taking all available cellular response information into account. Our method displays good performances in terms of bias, type I error control and statistical power under various scenarios in numerical simulations. We present a user-friendly R Shiny application (relying on the nlme package) that we implemented for immunologists to be able to use our method directly, and we applied it to analyze data from two HIV vaccine trials.",Bioinformatics & biostatistics,Ariane 1+2,Guillaume Devailly
Fri,10:50,Tools for 3D/4D interactive visualisation of cells and biological tissue,Marion Louveaux,"Modern microscopy techniques allow to image a high number of biological samples (cells, tissues, whole organs and organisms). Some tools even allow to follow a sample over time. Image processing techniques can extract many information from the images about biological objects (number, location in 2D or 3D space and time, biological status, shape...). Analysis of such biological data is facing to two main challenges: the need to take into account the spatial and temporal context of each piece of information, and the need to gather and compare many images pre-processed with multiple softwares from many samples. The R package {cellviz3d} (https://github.com/marionlouveaux/cellviz3d) is a wrapper on the {plotly} package to help visualization of meshes and points structures in 2D, 3D and 3D+time of one or several samples. Data can be extracted directly from images in R or with image analyses softwares like Image/Fiji or MorphoGraphX. Typical use cases that will be presented in this talk are the visualization of the surface of a biological tissue or the visualization of nuclei. Data presented will be data outputs read with packages like {mgx2r}, which bridges MorphoGraphX with R, or {mamut2r}, which bridges MaMut Fiji plugin with R, both available on GitHub (@marionlouveaux).",Bioinformatics & biostatistics,Ariane 1+2,Guillaume Devailly
Fri,10:55,Analysis of laboratory test requests in a university hospital: A Shiny App for association analysis as a demand management tool,Deniz Topcu,"Increasing medical expenses throughout the world necessitate more reasonable utilization of all healthcare resources, as well as clinical laboratory tests. Laboratory tests are known to be ordered either to screen individuals for diseases or to confirm the diagnosis of individuals with disease symptoms. Diagnostic tests should be utilized appropriately according to current guidelines and clinical algorithms. Overutilization of tests is considered as incorrect practices in both medical and financial contexts. In this study, to assess utilization of test requests in a hospital environment, we applied apriori based method of association analysis to determine the relationship between different laboratory tests and clinical settings. Using the apriori and shiny package we have development interactive application. Our application is user-friendly and users who are not familiar with R programming or statistical software can import data from Laboratory Information System and oversee test utilization. Managing test requests is an emerging topic in clinical laboratory; data science tools can help laboratory medicine specialists to be more proactive.",Bioinformatics & biostatistics,Ariane 1+2,Guillaume Devailly
Fri,11:30,Machine Learning Infrastructure at Netflix,Savin Goyal,"At Netflix, our data scientists apply machine learning to an ever-increasing range of business problems, from title popularity predictions to quality of streaming optimizations. However, building and operating production grade machine-learning systems is a highly non-trivial endeavor requiring expertise in two highly evolved disciplines: systems engineering and ML. To empower our data scientists to be able to build, deploy and operate machine learning solutions without any external support, we provide a robust infrastructure for machine learning, ensuring models can be promoted quickly and reliably from prototype to production, and enabling reproducible and easily shareable results. Given the breadth of techniques and the state-of-the-art algorithms available within the R ecosystem, a significant fraction of our data scientists, prefer R as their lingua franca, necessitating the need to provide first-class infrastructure support in R. In this talk, we introduce the techniques and underlying principles driving our approach with a particular focus on models written in R. We talk about Metaflow, our human-centric machine learning infrastructure, which enables our users to iterate on their machine learning pipelines quickly by leveraging existing libraries and familiar abstractions.",Model deployment,Cassiopée,Kevin Kuo
Fri,11:48,Deploying machine learning models at scale,Angus Taylor,"Data scientists face multiple challenges when deploying machine learning models at the scale required for many commercial applications. In this talk, I will discuss best practices for deploying R models in production, including: how to operationalize predictive workflows with event- or frequency-based scheduling; how to scale model training and scoring operations with cloud-based clusters of virtual machines; how to maintain parity between development and production environments using Docker containers; the use of continuous integration and continuous delivery (CI/CD) tools to build and test production code. I will present a demo from a retail product forecasting context, in which R models are deployed in an end-to-end predictive workflow on cloud-based architecture.",Model deployment,Cassiopée,Kevin Kuo
Fri,12:06,Serverless Computing for R,Christoph BodnerThomas Laber,"R is a great language for rapid prototyping and experimentation, but putting an R model in production is still more complex and time-consuming than it needs to be. With the growing popularity of serverless computing frameworks that offer Functions-as-a-Service (like AWS Lambda, Azure Functions) or Container-as-a-Service (ECS and ACI) we see a a huge chance to allow R developers to more easily deploy their code into production. We want to show you how you can use serverless computing to easily put models in production. We will discuss the pros and cons of various approaches and how we implemented a completely serverless data science platform for R in Microsoft Azure that you can arbitrarily scale (as long as your credit card allows:) up and down. While we come from an Azure background, porting the ideas over to AWS or Google Cloud should be straight forward.",Model deployment,Cassiopée,Kevin Kuo
Fri,12:24,A DevOps process for deploying R to production,David Smith,"So you've built an amazing model in R. It generates great predictions or recommendations on your desktop. Now, how do you get it into production? Deploying R functions within Docker containers, and exposing them with the 'plumber' package, is a simple and effective way to integrate R into applications via a REST API. We can further provide scale for high-volume workloads by deploying that container to Kubernetes. As the complexity grows, however, managing and updating deployments can be challenging. In this talk, we describe a CI/CD based process in Azure DevOps to automate the entire build, test and deploy process for R-based models in production. The process emphasizes model reproducibility, by capturing and tracking changes in the code, data, tests and configurations that define the model. You will learn how, with a check-in to GitHub as the trigger, your model can be automatically retrained, optimized, built, validated, and – with human approval if necessary – released. Once deployed to the production environment – in this talk we'll focus on scalable open-source frameworks like Kubernetes – the model will be subject to continuous monitoring and performance tracking until such time that a new model version is warranted, and the DevOps lifecycle begins again.",Model deployment,Cassiopée,Kevin Kuo
Fri,12:42,Authentication and authorization in plumber with the sealr package,Friedrike Preu,"Application Programming Interfaces (APIs) have become the most common way how services ""talk"" to each other, e.g. in a typical frontend-backend design. The plumber package offers capabilities to implement an API in R, making it possible to use R for software development use cases that often require security best practices. In my talk, I will show how we can use plumber filters to secure plumber APIs. I then present the sealr package (github.com/jandix/sealr) which provides standardized strategies for authentication and authorization, namely JSON Web Tokens (JWT) (implemented), OAuth2 and general API tokens (under development at time of submission). sealr is inspired by the manifold passport.js package for Node.js. The main functionality of sealr is verifying tokens sent to plumber - how those tokens are issued by plumber is not covered (yet) as it is highly application-specific. However, we provide implementation examples for each method. As authentication middleware specifically developed for the plumber framework, sealr differs from packages such as sodium, openssl, jose and bcrypt that implement specific encryption and/or hashing algorithms. secret, digest and keyring are used for securely storing (R) objects. With sealr, we aim to make authentication and authorization as easy as possible to implement for R users so that plumber APIs can be used in security-sensitive environments.",Model deployment,Cassiopée,Kevin Kuo
Fri,11:30,pak: a fresh approach to package installation,Gábor Csárdi,"pak is a new package manager that makes package installation fast, safe and convenient. Fast   Fast downloads and HTTP queries. pak performs all HTTP requests concurrently.   Fast installs. pak builds and installs packages concurrently.   Metadata and package cache. pak caches package metadata and all downloaded packages locally. It does not download the same package files over and over again.   Lazy installation. pak only installs the packages that are really necessary for the installation.   Safe   Private library (pak's own package dependencies do not affect your regular package libraries and vice versa).   Every pak operation runs in a sub-process, and the packages are loaded from the private library. pak avoids loading packages from your regular package libraries.   pak warns and requests confirmation for loaded packages.   Dependency solver. pak makes sure that you end up in a consistent, working state of dependencies. If finds conflicts up front, before attempting installation.   Convenient   BioC packages. pak supports Bioconductor packages out of the box. It uses the Bioconductor version that is appropriate for your R version.   GitHub packages. pak supports GitHub packages out of the box. It also supports the Remotes entry in DESCRIPTIONfiles, so that GitHub dependencies of GitHub packages will also get installed.    Package sizes. For CRAN packages pak shows the total sizes of packages it would download.",Performance,Saint-Exupéry,Helena Kotthaus
Fri,11:48,Summary of developments in R's data.table package,Arun Srinivasan,"data.table is an R package that allows for fast and memory efficient data manipulation with a concise and flexible syntax. It has over 8000 questions on StackOverflow, is frequently a featured package and one of the most downloaded packages (https://www.r-pkg.org), has over 60 contributors (https://github.com/Rdatatable/data.table/graphs/contributors) and is active in development for over 10 years.
Over the last couple of years, several new functionalities and improvements, including parallelisation of several operations, have been made including file reading (`fread`), file writing (`fwrite`), subset operations, automatic secondary indexing, ordering, non equi joins, overlapping joins, rolling functionalities etc.
In this talk, all such improvements will be summarised showcasing different tasks of varying complexity that can be tackled using data.table with ease and with little code. The intuition behind the syntax of data.table will also be explained.
Finally, a quick benchmark of some common tasks will be highlighted to clearly show the performance of data.table. This is particularly useful to R users who might benefit a lot from performance.
Main author: Matt Dowle, hacker at H2O.ai
Co-author: Arun Srinivasan (me), Analyst, Millennium Management.
Project page: https://github.com/Rdatatable/data.table/wiki",Performance,Saint-Exupéry,Helena Kotthaus
Fri,12:06,Real-time file import with the vroom package,Jim Hester,"File import in R could be considered a solved problem, with multiple widely used packages (data.table, readr, and others) providing fast, robust import of common formats in addition to the functions available in base R. However I feel there is still room for improvement in existing approaches. vroom is able to index and then query multi-Gigabyte files, including those with categorical, text and temporal data, in near real-time. This is a huge boon for interactive data analysis as you can jump directly into exploratory analysis without sampling or long waits for full import. vroom leverages the Altrep framework introduced in R 3.5 along with lazy, just-in-time parsing of the data to provide this improved latency without requiring changes to existing data manipulation code. I will throughly explain the techniques used in vroom to ensure good performance, describe challenges overcome in implementing it, and provide an interactive demonstration of its capabilities.",Performance,Saint-Exupéry,Helena Kotthaus
Fri,12:24,A Future for R: Simplified Parallel and Distributed Processing,Henrik Bengtsson,"In this talk, I'll present the future framework, how to use it, what is new, and what is on the roadmap. The future ecosystem provides a simple, unified framework for parallel and distributed processing in R. It allows you to ""write parallel code once"" regardless of the operating system and compute environment. At the same time, it allows the user to decide on where and how to parallelize your code. In programming, a 'future' is an abstraction for a value that may be available at some point in the future. The non-blocking nature of futures makes them ideal for asynchronous evaluation of expressions in R, e.g. in parallel on the local machine, on a set of remote machines, via an HPC job scheduler, or in the cloud. At its core, there are two construct: 'f",Performance,Saint-Exupéry,Helena Kotthaus
Fri,12:42,FastRCluster: running FastR from GNU-R,Stepan Sindelar,"FastR is an open source alternative R implementation that aims to be compatible with GNU-R, provide significantly faster execution of R code, and high-performance interoperability with other languages including Python. Although FastR can run complex R packages including Rcpp, dplyr or ggplot2, achieving near full compatibility with GNU-R is still work in progress. However, there is already a lot that FastR can offer, but switching completely to FastR may look like a big step. The goal of FastRCluster is to provide the intermediate ground and let the GNU-R users easily try FastR and/or use FastR to run only selected parts of their apps. FastRCluster is an R package, targeted at GNU-R, that provides a seamless way to run FastR inside GNU-R using the existing interface of the parallel package, which also integrates well with the future and promise packages. The talk will, for example, present how FastRCluster can be leveraged in the context of Shiny applications.",Performance,Saint-Exupéry,Helena Kotthaus
Fri,11:30,prVis: a Novel Method for Visual Dimension Reduction,Norman MatloffTiffany JiangWenxuan ZhaoRobert Tucker,"Visualization is an invaluable tool for gaining insight into complex high-dimensional data. Here we introduce prVis: a polynomial-based 2-D visualization method. PrVis, or polynomial visualization, captures potential nonlinear relationships in datasets by performing polynomial expansion and applying PCA on the resulting expanded polynomials. Our method produces an arguably better outcome on the famous Swiss Roll test case than do t-sne, UMAP, and ordinary PCA, and generally does as well or better than those methods on other datasets. PrVis offers options to further investigate how the clusters of data relate to the original variables. The prVis software package includes several features to support easier interpretation of visualized data, including color coding for discrete and continuous variables, alpha blending, adding row numbers to data points, and removing outliers. PrVis also provides users with options for random sampling and memory-mapping to better work with large datasets.",Big/high dimensional data,Caravelle 2,Pierre Neuvial
Fri,11:48,PLS for Big Data: A Unified Parallel Algorithm for Regularized Group PLS,Benoit Liquet,Partial Least Squares (PLS) methods have been heavily exploited to analyze the association between two blocks of data. These powerful approaches can be applied to data sets where the number of variables is greater than the number of observations and in the presence of high collinearity between variables. Different sparse versions of PLS have been developed to integrate multiple data sets while simultaneously selecting the contributing variables. Sparse modeling is a key factor in obtaining better estimators and identifying associations between multiple data sets. The cornerstone of the sparse PLS methods is the link between the singular value decomposition (SVD) of a matrix (constructed from deflated versions of the original data) and least squares minimization in linear regression. We review four popular PLS methods for two blocks of data. A unified algorithm is proposed to perform all four types of PLS including their regularised versions. We present various approaches to decrease the computation time and show how the whole procedure can be scalable to big data sets. The bigsgPLS R package implements our unified algorithm and is available at https://github.com/matt-sutton/bigsgPLS,Big/high dimensional data,Caravelle 2,Pierre Neuvial
Fri,12:06,multiDA and genDA: Discriminant analysis methods for large scale and complex datasets,Sarah Romanes,"Classification problems involving high dimensional data are extensive in many fields such as finance, marketing, and bioinformatics, with unique challenges with such datasets being numerous and well known. We first introduce the R package multiDA (https://github.com/sarahromanes/multiDA), a new method of performing high dimensional discriminant analysis. Starting from multiclass diagonal discriminant analysis classifiers which avoid the problem of high dimensional covariance estimation we construct a hybrid model that seamlessly integrates feature selection components. We compare our method with several other statistical machine learning packages in R, showing marked improvements in regard to prediction accuracy, interpretability of chosen features, and fast run time. We then introduce genDA (in active development), a DA method capable for use in multi-distributional response data - generalising the capabilities of DA beyond Gaussian response. It utilises Generalised Linear Latent Variable Models (GLLVMs) to capture covariance structure between the different response types and provide an efficient classifier for such datasets. This model leverages the highly efficient TMB package in R for fast and accurate gradient calculations in C++.",Big/high dimensional data,Caravelle 2,Pierre Neuvial
Fri,12:24,compboost: Fast and Flexible Component-Wise Boosting Framework,Daniel Schalk,"In high-dimensional prediction problems, especially in the case where the number of features exceeds the number of observations, feature selection is an essential and often required tool. Component-wise gradient boosting is a modelling technique which provides embedded feature selection for additive statistical models. It allows automatic and unbiased selection of ensemble components from a pool of - often univariate - base learners. Boosting these kinds of models maintains interpretability of effects.  The R package compboost implements component-wise boosting in C++ and Rcpp. It provides a modular object-oriented system which can be extended either in R for convenient prototyping or directly in C++ for optimized speed, the latter at runtime without full recompilation of the framework. The package also allows visual inspection and selection of effects, feature importance, risk behaviour and optimal number of iterations. In contrast to the well-known mboost package, compboost is more suitable for larger datasets and also easier to extend, whereas compboost currently lacks some of the large functionality mboost provides.  Project page: https://github.com/schalkdaniel/compboost",Big/high dimensional data,Caravelle 2,Pierre Neuvial
Fri,12:42,How to speed-up VSURF (Variable Selection Using Random Forests)?,Robin Genuer,"The VSURF package is a general tool to perform variable selection for regression or supervised classification problems. It implements a fully data-driven variable selection procedure based on random forests (RF). Its overall run-time depends obviously on the data characteristics (number of observations, n, and number of variables, p, mostly), but also on the computational performance of the randomForest package (since VSURF is heavily based on it). In the last few years, other packages implementing RF (ranger, Rborist) have emerged, and their authors claim that they are faster than randomForest: in the high-dimensional case (p very large and larger than n) for ranger, and in the Big data context (n extremely large) for Rborist. Therefore, one way to speed-up VSURF is to use those other packages instead of randomForest in the procedure. In this talk, RF basics are first given, then the VSURF procedure is detailed, and finally different implementations of RF into VSURF are compared in several frameworks and on different examples, to shed light on when each implementation can or cannot alleviate the overall computation burden.",Big/high dimensional data,Caravelle 2,Pierre Neuvial
Fri,11:30,"timeseriesdb - Manage, Process and Archive Time Series with R and PostgreSQL",Matthias Bannert,"The timeseriesdb framework maps R time series representations to PostgreSQL key-value pair storage and links data descriptions in a relational manner. Combining key-value pairs with relational database features results in a light-weight but powerful architecture that keeps maintenance at a minimum as it allows to store a large number of records without the need for multiple partitions. The timeseriesdb framework was tailored to the needs of official and economic statistics with long term data conservation in mind: It handles data revisions (vintages), release dates and elaborate, multi-lingual meta information. Apart from implementation insights, this talk discusses how data management in a well etablished, enterprise level, relational database as opposed to file based management opens up the opportunity to use standard web technologies such as REST to easily build interfaces and expose time series data online.",Time series data,Ariane 1+2,Achim Zeileis
Fri,11:48,A feast of time series tools,Rob Hyndman,"Modern time series are often high-dimensional and observed at high frequency, but most existing R packages for time series are designed to handle low-dimensional and low frequency data such as annual, monthly and quarterly data. The feasts package is part of new collection of tidyverts packages designed for modern time series analysis using the tidyverse framework and structures. It uses the tsibble package to provide the basic data class and data manipulation tools. feasts provides Feature Extraction And Statistics for Time Series, and includes tools for exploratory data analysis, data visualization, and data summary. For example, it includes autocorrelation plots, seasonality plots, time series decomposition, tests for units roots and autocorrelations, etc. I will demonstrate the design and use of the feasts package using a variety of real data, highlighting its power for handling large collections of related time series in an efficient and user-friendly manner.  ",Time series data,Ariane 1+2,Achim Zeileis
Fri,12:06,tsbox: Class-Agnostic Time Series,Christoph Sax,"The R ecosystem knows a vast number of time series classes: ts, xts, zoo, tsibble, tibbletime or timeSeries. The plethora of standards causes confusion. As different packages rely on different classes, it is hard to use them in the same analysis. tsbox provides a set of tools that make it easy to switch between these classes. It also allows the user to treat time series as plain data frames, facilitating the use with tools that assume rectangular data.
The package is built around a set of functions that convert time series of different classes to each other. They are frequency-agnostic, and allow the user to combine multiple non-standard and irregular frequencies. Because coercion works reliably, it is easy to write functions that work identically for all classes. So whether we want to smooth, scale, differentiate, chain-link, forecast, regularize or seasonally adjust a time series, we can use the same tsbox-command for any time series class. The talk provides a general overview of time series classes in R and shows how tsbox can be used to facilitate the interaction between them.",Time series data,Ariane 1+2,Achim Zeileis
Fri,12:24,RJDemetra: an R interface to JDemetra+ seasonal adjustment software,Alain Quartier-La-Tente,"More and more infra-annual statistics are produced to evaluate the short-term evolution of an economy (GDP, chocolate sales...) or even to analyse in detail the downloads of CRAN packages! Most of these series are affected by seasonal and trading days effects that must be removed to perform temporal and spatial comparisons. RJDemetra (https://github.com/jdemetra/rjdemetra and soon on CRAN) is an R interface to JDemetra+, the European seasonal adjustment software officially recommended by Eurostat and the European Central Bank and used by more than 80 institutes in the world. This package offers access to all options and outputs of JDemetra+ that implements the two leading seasonal adjustment methods TRAMO/SEATS and X-13ARIMA-SEATS and their pre-adjustment methods (automatic RegARIMA modelling, outlier detection, trading days adjustment). The presentation will highlight the main options of RJDemetra and show how all the resources available in R can be used to improve the production of seasonal adjusted series (producing dashboards, quality reports...) or to carry out studies.",Time series data,Ariane 1+2,Achim Zeileis
Fri,12:42,Experiences from dealing with missing values in sensor time series data,Steffen Moritz,"Sensors are omnipresent in our modern world. From manufacturing industry, finance, up to biology in nearly every domain sensors provide us with essential information. Although everybody seems to be well-equipped with sensors, operating these can be error prone. Especially missing values are a problem often encountered. Since follow-up analysis and processes often require complete data, missing data replacement ('imputation') is needed as a preprocessing step. In this talk we will give insights about our experiences on handling missing data in several industry related projects. We will also give a short introduction to the imputeTS package, which we developed as a result of these experiences.  Imputation itself is a well-established area in statistical surveys and CRAN offers a surprisingly large choice of packages (i.a. 'mice', 'missMDA', 'Amelia'). However, there are differences between imputation for statistical surveys and sensor time series. We will discuss typical problems arising with missing data imputation in sensor time series and also present for which kind of data / missing data patterns / missing data mechanisms imputation proved to be successful. Overall, this talk is supposed to provide a first glance at sensor data imputation in R along with giving an introduction to the imputeTS package.",Time series data,Ariane 1+2,Achim Zeileis
Fri,11:30,How to win friends and write an open-source book,Jakub NowosadRobin Lovelace,"Over the last few years, a quiet revolution has been brewing in the R book publishing industry. Since the publication of early open source books, such as Advanced R (published in 2014), many authors have switched to a hybrid system, in which books are published online and in print. This approach has several advantages, including (1) people can choose how to read the book, online or in print, and make an informed decision before buying it; (2) the book retains the process of reviews and professional copy-editing provided by publishers; (3) the wider community can contribute, leading to many improvements in the code and text. Several developments have enabled this revolution, including the bookdown package, online version control services (e.g., GitHub), continuous integration services (e.g., Travis CI), and many more. This talk will share our experience of writing the open source book Geocomputation with R (https://geocompr.github.io/). We will show how to start writing an open-source book, which tools are useful, and how to collaborate on a large project while being a continent apart. We will also point out a few issues in the process, and how we navigated them. Overall, we advocate writing an open source book and hope the talk will be useful to others thinking about starting such a project.",Contribution & collaboration,Concorde 1+2,Hadley Wickham
Fri,11:48,Making sense of CRAN: Package and collaboration networks,Ioannis Kosmidis,"The Comprehensive R Archive Network (CRAN) is a diverse software ecosystem that is growing fast in size, both in terms of packages and package authors. The numbers at the time of writing this abstract are that CRAN involves more than 13.7K packages from about 19.2K authors. Keeping track of the ecosystem is, at least to me, hard, particularly in terms of effectively searching packages and authors through the network, summarising it and discovering new package and author associations. In this talk, I will introduce the cranly R package (https://github.com/ikosmidis/cranly) which streamlines and simplifies the analysis of the CRAN ecosystem. In particular, we will cover cranly's capabilities for cleaning up and organising the information in the CRAN package database, building package directives networks (depends, imports, suggests, enhances, linking to) and collaboration networks, and computing summaries and producing interactive visualisations from the resulting networks. I will also touch upon modelling, particularly on how directives and collaboration networks can be coerced to igraph (https://CRAN.R-project.org/package=igraph) objects for further analyses and modelling.",Contribution & collaboration,Concorde 1+2,Hadley Wickham
Fri,12:06,RWsearch: a package for CRAN users and task view maintainers,Patrice Kiener,"Navigating the R Package Universe was intensively discussed at useR! 2017 and is still a hot topic. Ordinary R users expect to find the best packages that match their needs whereas task view maintainers have to identify all packages that fall in the scope of their task views. With the size of CRAN, the task is immense and a dedicated search engine is required. Some new solutions appeared in 2018 and 2019 and will be discussed in this talk.   Among them, the 'RWsearch' package was developed to facilitate the maintenance of the 'Distributions' task view (which is ranked number five by the number of referred packages) but can be used by every R users. It includes a comprehensive set of functions to download the packages and task view master files from CRAN, explore these files, search packages by keywords, date of publication and sophisticated options, display the results as a data.frame or as a classical text in console, txt, html or pdf outputs, download the whole package documentation in one click (a wonderful function for people who need to work offline) and, for task views, compare the results with the list of already referred packages. Since its inception, 'RWsearch' has helped us discover more than 100 new packages.   Along with the CRAN exploration tools, 'RWsearch' has direct links to more than 60 external web search engines. We expect this list to grow with the contribution of everyone.",Contribution & collaboration,Concorde 1+2,Hadley Wickham
Fri,12:24,"Translating datasets using ""datalang"": the development of ""datos"" package for the R4DS Spanish translation",Riva Quiroga,"Not mastering English language can be a very high entrance barrier for people who want to learn how to program, specially in contexts like Latin America, where learning English is not accessible to everyone due to socioeconomic inequalities.  The aim of this talk is to present the collaborative translation to Spanish of ""R for Data Science"" (Whickham & Grolemund, 2017), a project that the Latin American R community is currently leading as a way to shorten this linguistic gap. Unlike other translations, this one not only considers translating the text to Spanish, but also the datasets and part of the code used in the book. After briefly describing the general context of the project, the presentation will focus on how we developed ""datos"" (Ruiz, Quiroga, Vargas & Lepore, 2019; https://cienciadedatos.github.io/datos/), the package that contains the R4DS datasets translated. The talk will present the motivations to develop this package, the workflow used for programatically translate the datasets using ""datalang"" package (Ruiz, 2018; https://github.com/edgararuiz/datalang), and the challenges we have faced. Opportunities for other non-English speaking R communities will be discussed.",Contribution & collaboration,Concorde 1+2,Hadley Wickham
Fri,12:42,R Consortium Working Groups,Joseph Rickert,"R Consortium (ISC) working groups are where the difficult collaborative work gets done, and there is a lot going on! In this talk, I will describe what is happening in some of the high profile working groups including the R / Medicine and R / Pharma conference planning groups; the R Validation Hub group that is developing validation standards for R packages for the pharmaceutical industry; The Census group that is working with the U.S. Census Bureau to improve the accessibility of Census Data, the R Certification group, which is attempting to establish a common certification program for proficiency in R; and the R Community Diversity and Inclusion Working Group (RCDI-WG) is to broadly consider how the R Consortium can best encourage and support diverse and inclusive community activities. Working groups are generally open to all interested individuals whether or not their companies are members of the R Consortium. I'll describe how you can get involved, or perhaps even start your own working group.  ",Contribution & collaboration,Concorde 1+2,Hadley Wickham
Fri,11:30,Implementation and analysis design of an adaptive-outcome trial in R,Alessio Crippa,"The development of new drugs in oncology often requires the identification of predictive biomarkers for patients where a specific drug is more beneficial. The multiplicity of available treatments as well as the vast collection of biomarkers makes the design of clinical trials difficult. A platform trial with an adaptive design allows the investigator to learn from the accumulating data and to modify prespecified components of the trial while it is ongoing. This can improve power, reduce the number of patients and costs, treat more patients with effective therapies, correctly identifying subgroup of responding patients, and shortening the time of the trial. However, adaptive designs are more complex than traditional studies and require particular attention in their design and analysis. I will present the design and implementation of the ProBio study, a biomarker driven platform trial for improving treatment decision in patients with metastatic castrate resistant prostate cancer. I will describe how to assess the operating characteristics (type I error and power) in R through extensive simulations and calibrations. I will illustrate Bayesian methods for survival analysis employed in the evaluation of the accumulating data, and outline the advantages of using R in the diverse phases of the trial, such as producing randomization lists, generating automatic reports, and relevant dashboards.",Biostatistics & epidemiology 2,Guillaumet 1+2,Mette Langaas
Fri,11:48,Advances in dose-response analysis,Christian RitzJens C. Streibig,"During the last few decades, dose-response analysis, which is fitting and interpreting results obtained from using fully parametric nonlinear dose-response (regression) models, has undergone dramatic changes, from cumbersome, more or less manual calculations and transformations to blink-of-an-eye operations on any laptop. The first version of the R extension package ""drc"" for doing dose-response analysis was developed in 2005. Originally, it was developed for fitting log-logistic models that were used routinely in toxicology and pesticide science. Subsequently, the package has been extensively revised, mostly in response to feedback from the user community. By now, it has developed into a veritable ecosystem for all kinds of dose-response analyses. Similar functionality for dose-response analysis does not exist in any other statistical software.
This talk briefly reviews the development, mentinoning methods that have become obsolete, methods that have stayed in use, and new methods that have been incorporated. Changes have largely been propelled by the advent of powerful extension packages. Specifically, we will talk about modular use of multiple packages when doing dose-response analysis. We will show examples on modelling of event times, binary mixtures and species sensitivity distributions. Finally, we will outline a number of directions for future developments.",Biostatistics & epidemiology 2,Guillaumet 1+2,Mette Langaas
Fri,12:06,The next generation of the survival package,Terry Therneau,"With over 650 dependent packages, two guiding lights for the survival package are that ""you can't do everything"" and ""don't break it"". As a consequence, although there has been continuous improvement in the underlying algorithms, additions of major new functionality are rare. Version 3 is an exception. Major additions include much broader support for multi-state models, cacluation of absolute risk estimates, and data checking. A design goal was to make multi-state models as easy to fit and use as a single state coxph or survfit model, and to make absolute risk estimates as easy as a Kaplan-Meier curve --- all without rocking the boat with respect to current software. Why do this? The classic triad of Kaplan-Meier, log-rank, and Cox model are reliable tools for time-to-event data that has a single endpoint. However, in the authors' own work single endpoint models have now become the minority. We will give an example from the Mayo Clinic Study of Aging where keeping track of multiple endpoints is a key part of the analysis: the progression from cognitively normal (CN) to mild cognitive impairment (MCI) to dementia; low/medium/high levels of underlying neurodegeneration, the accumulation of amyloid plaques in the brain, and of course status of alive/dead. We will illustrate the new usages, and discuss how other packages can plug into this functionality and where to find further documentation and examples.",Biostatistics & epidemiology 2,Guillaumet 1+2,Mette Langaas
Fri,12:24,A flexible approach to time-to-event data analysis using case-base sampling,Jesse Islam,"In epidemiological studies of time-to-event data, a quantity of interest to the clinician and the patient is the absolute risk of an event given a covariate profile. However, time matching or risk set sampling (including Cox partial likelihood) eliminates the baseline hazard from the likelihood expression for the hazard ratios. This has to be recovered using a non-parametric method which leads to step-wise estimates of the cumulative incidence that are difficult to interpret. The analysis can be further complicated in the presence of competing events. Using case-base sampling, we explain how a single event type or competing risk analysis that directly models the hazard function parametrically can be performed using logistic or multinomial regression. This formulation thus allows us to leverage all the machinery developed for GLMs including goodness-of-fit tests and variable selection. Furthermore, because we are explicitly modeling time, we obtain smooth estimates of the cumulative incidence that are easy to interpret. We demonstrate our approach using data from patients who received stem-cell transplant for acute leukemia. We provide our method in the casebase R package published on CRAN with extensive documentation and examples.",Biostatistics & epidemiology 2,Guillaumet 1+2,Mette Langaas
Fri,12:42,The R package mixmeta: an extended mixed-effects framework for meta-analysis,Antonio Gasparrini,"The package mixmeta offers an extended mixed-effects framework for meta-analysis, where potentially complex patterns of effect sizes are modelled through a flexible structure of fixed and random terms. This allows the definition of standard univariate fixed and random-effects models, but also extensions separately proposed in the literature, such as multivariate, network, multilevel, longitudinal, and dose-response meta-analysis and meta-regression. The main function mixmeta provides a simple way to specify alternative models through a formula syntax consistent with standard linear mixed-effects packages. Likelihood-based estimation procedures are implemented using an efficient and robust hybrid approach consisting of a combination of iterative generalized least squares and Newton-type algorithms. Methods functions for standard regression tasks and specific meta-analytical summaries. The package is thoroughly documented and includes illustrative examples that replicate published analyses using a variety of meta-analytical models.",Biostatistics & epidemiology 2,Guillaumet 1+2,Mette Langaas
Fri,14:15,'AI for Good' in the R and Python ecosystems,Julien Cornebise,"Drawing from fifteen years of concrete examples and true stories, from algorithmic uses (computational Bayesian statistics, then deep learning and reinforcement learning at DeepMind) to more recent and more applied ""AI for Good"" projects (joint with Amnesty International: detecting burned villages on satellite imagery in conflict zones, or studying abuse against women on Twitter, etc), we discuss here how the tools of several communities sometimes rival and sometimes complement each other - in particular R and Python, with a bit of Lua, C++ and Javascript thrown into the mix. Indeed, from mathematical statistics to computational statistics to data science to machine learning to the latest ""Artificial Intelligence"" effervescence, our large variety of practices are co-existing, overlapping, splitting, merging, and co-evolving. Their scientific ingredients are remarkably similar, yet their technical tools differ and can seem alien to each other. By the end of this talk, we hope each attendee will leave with, in the worst case some interesting stories, in a better case some more thoughts on how this multitude of approaches play together to their own strengths, and, in the best case, some intense conversations in the Q&A!",Keynote,Concorde 1+2,Erin Le Dell
