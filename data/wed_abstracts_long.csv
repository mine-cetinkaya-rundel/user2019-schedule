Day,Time,Title,Speaker,Abstract,Session,Room,Chair
Wed,10:00,R for better science in less time,Julia Stewart Lowndes,"There is huge potential for R to accelerate scientific research, since it not only provides powerful analytical tools that increase reproducibility but also creates a new frontier for communication and publishing when combined with the open web. However, a fundamental shift is needed in scientific culture so that we value and prioritize data science, collaboration, and open practices, and provide training and support for our emerging scientific leaders. I will discuss my work to help catalyze this shift in environmental science through the Ocean Health Index project. Over the past six years, our team has dramatically improved how we work by building an analytical workflow with R that emphasizes communication and training, which has enabled over 20 groups around the world to build from our science and code for ocean management. R has been so transformative for our science, and we shared our path to better science in less time (Lowndes et al. 2017) to encourage others in the scientific community to embrace data science and do the same. Building from this, as a Mozilla Fellow I recently launched Openscapes to engage and empower scientists and help ignite this change more broadly.",Keynote,Concorde 1+2,Julia Silge
Wed,11:30,Logging and Analyzing Events in Complex Shiny Apps,Kamil Wais,"The ""shinyEventLogger"" package is a logging framework dedicated to complex shiny apps. Its main goal is to help to develop, debug, and analyze usage of our apps. Multiple-line events logging can be done simultaneously, not only to R console, but also to your browser JavaScript console, so you can see logged events in the real-time, using your app already deployed to a server (shinyapps.io, rsconnect). Moreover, your events (together with unique sesssionID and timestamp) can be saved as a text file or into a remote database (currently MongoDB), and be ready for further analysis with the help of process-mining techniques from ""bupaR"" package. You can log different types of events, for example:
 *  `log_event(""Hello World!"")`
 * `log_value(input$variable)`
 * `log_output(str(mtcars))`
 * `log_test(testthat::expect_true(TRUE))`
 * and others (errors, warnings, messages). 
You can time nested events for performance analysis. If you are logging a value of an evaluated expression, not only the value will be logged but also the expression itself. Each event can be logged with a list of parameters that are event-specific or common for a group of events. 
CRAN: https://cran.r-project.org/package=shinyEventLogger 
Vignette: https://kalimu.github.io/shinyEventLogger/articles/shinyEventLogger.html  ",Shiny 1,Saint-Exupéry,Aline Deschamps
Wed,11:48,mwshiny: Connecting Shiny Across Multiple Windows,Hannah De Los Santos,"We present mwshiny, a package that extends Shiny applications across multiple windows. Shiny lets R users develop interactive applications, alleviating the need for web development languages. Increasingly, users have access to multi-monitor system configurations; further, with Shiny apps hosted online, the possibility of a remote controller driving a visualization-focused monitor provides another necessity for a multi-window environment. Using mwshiny, users set up the interface and functionality of these multi-window applications by using Shiny's familiar syntax and conventions. By elegantly utilizing Shiny's reactive structure, we break down app development into a simple workflow with three parts: user interface development, server computation, and server output. We demonstrate this workflow through three case studies, including the aforementioned multi-monitor and controller-driving situations, in which we focus on population dynamics and cultural awareness, respectively. We also show mwshiny as applied to an immersive healthcare visualization; the Rensselaer Campfire, a new interface designed for group interaction, consists of two connected monitors in the form of a cylindrical fire pit, as well as an outside controller. These case studies show the impact of mwshiny as we move into a future with ever more immersive applications and structures.",Shiny 1,Saint-Exupéry,Aline Deschamps
Wed,12:06,Shiny app deployment and integration into a custom website gallery,Riccardo PorrecaRoland Schmid,"R Shiny has become overwhelmingly popular and widespread in the R community. Shiny web applications provide companies and individuals with an excellent way of demoing and showcasing data analytics and visualization, and interactive R-based projects in general. Although products and services exist for exposing Shiny apps on the web (Shiny Server, RStudio Connect, Shinyapps.io, ShinyProxy), there is a desire for and a clear benefit of full integration with an existing website. This allows providers to retain full flexibility in terms of customization and the experience provided to users exploring their apps. In this talk, we cover the challenges and requirements for the seamless embedding and integration of Shiny apps into a custom gallery part of a larger website. We will show how we used Docker and Kubernetes as natural choices for deploying and running Shiny apps, and for customizing the way they are served and made accessible through the web. We explain how to set up an embeddable Shiny app, how to build, maintain and deploy the corresponding Docker image, and how to configure the relevant Kubernetes resources. Finally, we will demonstrate how all this can easily be combined into a GitHub Pages website based on Jekyll.",Shiny 1,Saint-Exupéry,Aline Deschamps
Wed,12:24,Automated Surveys and Reports for Expert Elicitation with Shiny,Machteld Varewyck,"Experts are often consulted to quantify uncertainty on subjects when insufficient data is available, e.g. the temperature in Toulouse next week. The goal is to find a consensus by combining all experts' knowledge. We have built an R Shiny application to automate this procedure for an international governmental organization. 
The administration module allows to create a survey, invite experts and control the elicitation phase. When elicitation is completed, a report can be built automatically. This helps to find a consensus by plotting e.g. the combined distribution of all experts' elicitations. The administrator can, at any time, download the available data for use outside the Shiny application.
In the elicitation module, the assigned experts can define values for the requested quantiles in a table. The corresponding distribution is shown in an interactive barplot. By dragging the breakpoints in the barplot, the provided elicitation values in the table are automatically updated and vice-versa. 
During this talk, we will share our experiences on deployment with ShinyProxy. This allowed us to easily restrict the administration module to authorized users only and to guarantee persistent data storage using a docker volume.",Shiny 1,Saint-Exupéry,Aline Deschamps
Wed,11:30,Enhancements to data tidying,Hadley Wickham,"The goal of the tidyr package is to help get your data into a ""tidy"" form, where variables are found in columns and observations are found in rows. tidyr has two main drawbacks:
* Many people (including me!) find it hard to remember exactly how spread() and gather() work, and most usage require re-reading the documentation. This suggests there are fundamental problems with their design.
* Many web APIs provide data in JSON, which turns into deeply nested lists when loaded into R. tidyr provides few tools for tidying or rectangling this sort of data.
In this talk, I will introduce new tools in tidyr 1.0.0 that aim to solve both problems, making it easier to tackle traditional rectangular reshaping, as well as making it easier to rectangle deeply nested lists into a convenient form.",Data handling,Concorde 1+2,Kirill Müller
Wed,11:48,n() cool #dplyr things,Romain Francois,"dplyr, which provides tools for data summary and transformation, is one of the key user-facing packages of the tidyverse. dplyr is so powerful because each function is small and does one thing well. But this can also make it hard to learn, because it's not obvious what you can do, and there are often non-obvious tricks that can make your life much easier. In this talk, I'll summarise() a set of n() cool tricks, arrange(desc(NEWS)) to highlight some of the recent changes, and give you glimpse() of some of our thinking about the future.",Data handling,Concorde 1+2,Kirill Müller
Wed,12:06,You don't need Spark for this - larger-than-RAM data manipulation with disk.frame,Zhuo Jia Dai,"R is blessed with the very best data munging tools such as dplyr and data.table. However, R requires data to be loaded to RAM in the form of a data.frame; and a corollary of this is that R cannot deal with larger-than-RAM data easily. This talk introduces the disk.frame package which is designed to manipulate ""medium"" data - those datasets that are too large to fit into RAM but can be manipulated on a single machine with the right tools. In a nutshell, disk.frame makes use of two simple ideas to make larger-than-RAM data manipulation feasible * split up a larger-than-RAM dataset into chunks and store each chunk in a separate file inside a folder and
* provide a convenient API to manipulate these chunks Furthermore, disk.frame optimises on-disk data manipulation * by using state-of-the-art data storage format provided by the fst package to efficiently read and write data to disk
* by parallelizing operations with the excellent future package
* by using the data.table package's fast algorithms for grouping and merging Finally, disk.frame supports many dplyr-verbs to make it accessible to useRs who are already familiar with dplyr.",Data handling,Concorde 1+2,Kirill Müller
Wed,12:24,Data frames for grouped data: the gdata.frame package,Yves Croissant,"The formula-data interface is a critical advantage of R in order to describe models to be estimated. However, more enhanced formula and data are often usefull. The new gdata.frame package tackle the case when the data set is characterized by two indexes. Two examples are panel data, for which observations are defined by an individual and a time period, and random utility models, for which observations are defined by a choice situation and an alternative. Moreover, indexes may have a nesting structure: for example, for a panel data of countries, the individual (country) index can be nested in a continent index. With gdata.frame, the indexes are stored as an attribute of the data.frame and extracted series inherit from it, thanks to specific methods for generic extractor functions. Usual operations, like for example group means or deviations from group means can then be computed in a very natural way, ie without having to define each time the series which defines the structure of the data set. gdata.frame therefore provides a generic solution to deal with grouped data. More specific data structure can then be defined that inherit from it. This feature is illustrated using the plm (for panel data) and the mlogit (for discrete choice models) packages.",Data handling,Concorde 1+2,Kirill Müller
Wed,12:42,git2rdata: storing dataframes in a plain text format suitable for version control,Thierry Onkelinx,"Base R has write.table() and read.table() to work with data in plain text files. Factors are stored as their labels, making them indistinguishable from characters and losing information on the levels. The git2rdata packages provides write_vc() which stores a dataframe as two plain text files: a tab separated file with the data and a metadata file in YAML format. The metadata stores the class of each variable and all other relevant metadata, e.g. the factor levels and their order. read_vc() reads the raw data using read.table() and restores the variables based on their metadata. Storing the metadata also allows to optimize the file storage, e.g. by storing factor indices rather than factor labels in the data file. This optimization can be turned of in case human readable data is preferred over smaller files. Git stores changes as row wise diffs. Swapping the order of two variables results in changing every single row of the plain text file. write_vc() avoids this be reordering the variables based on the existing metadata. Sorting the observations along user defined variables result in a stable row order. Metadata can be overridden if needed to accommodate a change in variables. git2rdata is useful to store and retrieve data in a reproducible workflow. Installation instructions, documentation and vignettes are available at https://inbo.github.io/git2rdata/",Data handling,Concorde 1+2,Kirill Müller
Wed,11:30,A Generalized Framework for Parametric Regression Splines,Georges MonetteJohn Fox,"Regression splines are piecewise polynomials constrained to join smoothly at boundaries called knots. They are traditionally viewed as an alternative to other methods for modeling nonlinear relationships, such as transformations, polynomial regression, and nonparametric regression. Regression splines are parametric and are implemented by introducing a regression-spline basis into the model matrix for a linear or similar regression model. It is usual not to focus on the estimated parameters for a regression spline but instead to represent the model graphically, and traditional regression-spline bases, such as B-splines and natural splines, respectively implemented in the bs() and ns() functions in the R splines package, are selected for numerical stability rather than interpretability. The emphasis on graphical interpretation makes sense but also represents a missed opportunity. We introduce generalized regression splines, implemented in the gspline() in the carEx package, which support the specification of a much wider variety of regression-spline and piecewise-polynomial models using bases that are associated with interpretable parameters.",Models 1,Caravelle 2,Torsten Hothorn
Wed,11:48,Regularized estimation of the nominal response model,Michela Battauz,"The nominal response model is an Item Response Theory (IRT) for polytomous items model that does not require a predetermined order of the response categories. While providing a very flexible modeling approach, it involves the estimation of many parameters at the risk of numerical instability and overfitting. The lasso is a technique widely used to achieve model selection and regularization. In this talk, we propose the use of a fused lasso penalty to group response categories and perform regularization. An adaptive version of the penalty is also considered. Simulation studies show that the proposal is quite effective in grouping the response categories, thus leading to a more parsimonious model. A remarkable advantage of the procedure is the reduction of both bias and root mean square error in small samples, while no difference is observed in large samples. An application to TIMSS data will illustrate the method. The R package regIRT (available at https://github.com/micbtz/regIRT) implements the methods.  ",Models 1,Caravelle 2,Torsten Hothorn
Wed,12:06,merlin - mixed effects regression for linear and nonlinear models,Alessandro Gasparini,"The rise in availability of electronic health record data raises both challenges and opportunities for new and complex analyses. The merlin package in R provides an extended framework for the analysis of a range of data types, encompassing any number of outcomes of any type, each of which could be repeatedly measured (longitudinal), with any number of levels and with any number of random effects at each level. Many standard distributions are described, as well as non-standard user-defined non-linear models. The extension focuses on a complex linear predictor for each outcome model, allowing sharing and linking between outcome models in a flexible way, either by linking random effects directly, or the expected value of one outcome (or function of it) within the linear predictor of another. Non-linear and time-dependent effects are also seamlessly incorporated to the linear predictor through the use of splines or fractional polynomials. merlin allows level-specific random effect distributions and numerical integration techniques to improve usability, relaxing the normally distributed random effects assumption to allow multivariate t-distributed random effects. We will take a single dataset of patients with primary biliary cirrhosis and attempt to show the full range of capabilities of merlin.",Models 1,Caravelle 2,Torsten Hothorn
Wed,12:24,Modern likelihood-frequentist inference with the likelihoodAsy package,Ruggero Bellio,"The talk illustrates the R package likelihoodAsy, available on CRAN and implementing some tools for higher-order likelihood inference. The basic functionality of the package is the implementation of the modified directed deviance for inference on a scalar parameter of interest, that could be seen as a fast method to approximate the most accurate parametric bootstrap inferences for the same task. The usage of the package requires to provide code for the likelihood function, for generating a sample from the model, and for the formulation of the interest parameter. The latter is allowed to be a rather general function of the model parameters. The code includes some functions for computing the modified profile likelihood for a multidimensional parameter of interest, and it could also be used to approximate median-unbiased estimation in a parametric statistical model. The features of the package will be illustrated by means of some examples on survival data models, IRT models and mixed models. ",Models 1,Caravelle 2,Torsten Hothorn
Wed,12:42,General-to-Specific (GETS) Modelling with User-Specified Estimators and Models,Genaro Sucarrat,"General-to-Specific (GETS) modelling provides a comprehensive, systematic and cumulative approach to statistical modelling ideally suited for hypothesis testing, forecasting and counterfactual analysis. The implementation of GETS-modelling, however, puts a large programming-burden on the user, and may require substantial computing power. We develop a framework for GETS-modelling with user-specified estimators and models, and provide flexible and computationally efficient functions via the R package gets (Pretis, Reade and Sucarrat (2018), J. of Statistical Software). In addition, the framework permits the user-specified estimation to be implemented in an external language (e.g.\ C++, Python, STATA, EViews, MATLAB, etc.).",Models 1,Caravelle 2,Torsten Hothorn
Wed,11:30,Application of WRSS in Water and Energy Analysis; An object oriented R package for large-scale water resources operation,Rezgar Arabzadeh,"Water Resources Simulator, an object-oriented open-source software package based on R language, is developed for simulation of water resources systems based on Standard Operation Policy. In spite of numerous commercially available software packages for modeling and simulation of water resources systems, only a limited number of free and open source tools are available in this regard. This acted as the initiative for the development of WRSS which enables water resources engineers to study and assess water resources projects by modeling and simulation. This package provides users a number of functions and methods to build a model, manipulate its components, simulate the scenarios and publish and visualize the results in a water resource system study. WRSS is capable to incorporate various components of a large and complex supply-demand system as well as hydropower analysis for reservoirs since they have not been available in any other R packages. In addition, a particular coding system, devised for WRSS, allows water resources component to interact together by transferring mass in terms of seepage, leakage, spillage, and return flow. In addition to its capabilities, the package was successfully applied to a case study of a water resources system composed of 5 reservoirs and 11 demand sites and the results proved the role of WRSS in the enhancement of the dam operation under the large-scale model.",Applications 1,Ariane 1+2,Wayne Jones
Wed,11:48,An R Package for the Distributed Hydrological Model GEOtop,Emanuele Cordano,"Eco-hydrological models are increasingly used in the contexts of hydrology, ecology, precision agriculture for better management of water resources and climate change impact studies at various scales: local, hillslope or watershed scale. However, with increasing computing power and observations available, bigger and bigger amounts of raw data are produced. Therefore, the need to develop flexible and user-oriented interfaces to visualize and analyze multiple outputs, e.g. performing sensitivity analyses, comparing and optimizing against observations (for specific research) or extraction of information (for data science), emerges. We present here the R open-source package **geotopbricks** (https://CRAN.R-project.org/package=geotopbricks), which offers an I/0 interface and R visualization and optimization tools for the GEOtop hydrological distributed model (https://www.geotop.org - GNU General Public License v3.0). This package aims to be a link between the work of environmental engineers, who develop hydrological models, and the ones of data and applied scientists, who can extract information from the model results. Applications related to the simulation of water cycle dynamics (model calibration, mapping, data visualization) in some alpine basins are shown.",Applications 1,Ariane 1+2,Wayne Jones
Wed,12:06,Big data analysis for power plant operational data for optimizing engineering design in R / Shiny,Friedrich-Claus Grueber,"A typical gas fired power plant is equipped with sensors and instrumentation devices to capture key operating parameters of power plants. With a large fleet of power plants operating around the globe over >= 10 years , the volume of sensor data being captured enters the realm of big data and provides an opportunity to apply big data analytic techniques to mine valuable information for optimizing engineering design of power plants.
The often used ""worst case approach"" usually leads to large design margins and increased cost. With the advent of big data technology, low cost computation and memory, it is now possible to mine the sensor measurements to either validate or dispute worst case assumptions made during design. Therefore a core processing R kernel was developed performing statistical and data analysis on time series data like:   KDEs, pdfs, correlations,Time series   probability estimations in regions    conditional probability analysis   violin/box plots   Event Detection    An Shiny application has been developed integrating the core processing kernel and providing data analytic functionality available for design engineers like automatic :
- shortlisting/specification of senor names
- generatiion of fitting list of plants & units of our fleet
- download and post processing of time series data 
- GUI for data analytic techniques/visualations",Applications 1,Ariane 1+2,Wayne Jones
Wed,12:24,Anomaly Detection in R,Priyanga Dilini Talagala,"Anomaly detection problems have many different facets that lead to wide variations in problem formulations. At present, there is a fairly rich variety of R software packages supporting anomaly detection tasks using different analytical techniques. Some of these use an approach to anomaly detection based on a forecast distribution. We locate over 75 R packages with anomaly detection capabilities via a comprehensive online search. We first present a structured and comprehensive discussion on the functionality and capability of these publicly available R packages for anomaly detection. Despite the large number of packages available, there are some anomaly detection challenges that are not supported with existing packages. We reduce this gap by introducing three new R packages for anomaly detection, `oddstream`, `oddwater` and `stray`, with special reference to their capabilities, competitive features and target applications. Package `oddstream` introduces a framework that provides early detection of anomalous behaviours within a large collection of streaming time series. This includes a novel approach that adapts to non-stationarity in the time series. Package `oddwater` provides a framework for early detection of outliers in water-quality data from in situ sensors caused by technical issues. Package `stray` provides a framework to detect anomalies in high dimensional data.",Applications 1,Ariane 1+2,Wayne Jones
Wed,12:42,Collective and Point Anomaly Detection in R,Daniel Grose,"Anomaly detection is a topic of considerable importance and has been subject to increasing attention in recent years. This is certainly true within the R community, as evidenced by the number of packages hosted by CRAN related to this area.
The majority of packages contain approaches aimed at detecting point anomalies i.e. single observations, which are anomalous with regards to their data context. We introduce the anomaly package, which contains novel methodology aimed at detecting not only point anomalies but also collective anomalies, i.e., anomalous data segments. Importantly, the main algorithm of the package is capable of simultaneously detecting and distinguishing between both anomaly types. The utility of this approach is demonstrated on both simulated data, and real astrophysical data from NASA's Kepler telescope, which is included within the package.",Applications 1,Ariane 1+2,Wayne Jones
Wed,11:30,Mathematical Modeling with R: Embedding Computational Thinking into High School Math Classes in the United States,Kenia Wiedemann,"Only 35% of high schools in the US offer computer science (CS). However, by 2020 50% of all new STEM jobs will require some knowledge in CS and coding. Developing computational thinking (CT) across all disciplines and educational levels have become a priority for scholars and agencies, urging to integrate CT across K-12 curricula. CT can be better learned when blended to primary subject areas. CodeR4MATH is an NSF-funded project working to provide a collection of learning and tutoring resources for math educators to collaborate in an R environment. Linking math to real-life problem solving, math modeling is an iterative process involving situation representation, math operations, interpretation, and validation. The modules are being developed to be used in high school math classes in Stats, Algebra I and II. We present here the results from the first module (named Lifehacking) implementation, showing students greatly benefit from investigating meaningful problems that appeal to their personal interests. It guides students to model in R to answer practical questions such as the costs of eating in college or the costs of owning a car. Tasks are delivered via R tutorials using learnr tools, with students using real data to create their models. They are then guided to move to RStudio exploring its capabilities. The 2nd and 3rd modules are under development, focusing on Earth Sciences and Engineering.",Education,Cassiopée,Matthias Gehrke
Wed,11:48,Teaching data science with puzzles,Irene Steves,"Of the many coding puzzles on the web, few focus on the programming skills needed for handling untidy data. During my summer internship at RStudio, I worked with Jenny Bryan to develop a series of data science puzzles known as the ""Tidies of March."" These puzzles isolate data wrangling tasks into bite-sized pieces to nurture core data science skills such as importing, reshaping, and summarizing data. We also provide access to puzzles and puzzle data directly in R through an accompanying Tidies of March package. I will show how this package models best practices for both data wrangling and project management.",Education,Cassiopée,Matthias Gehrke
Wed,12:06,Teaching R and statistics to higher degree research students and industry professionals,Alethea Rea,"The University of Western Australia's Centre for Applied Statistics has for many years offered short courses for higher degree research students and industry-based professionals. In 2019 we have released stage one of our new programme of courses, all of which are based in R using the tidyverse approach. This presentation will describe our experience of moving from courses in base R to a tidyverse approach, highlighting advantages and disadvantages for us as educators and our students as learners. We will also describe our current programme, planned future courses and the pathways for participants based on their statistical background.",Education,Cassiopée,Matthias Gehrke
Wed,12:24,ghclass: an R package for managing classes with GitHub,Colin Rundel,In this talk we will present the details of the ghclass package which we have developed as a tool for managing statistical and data science courses with a computation component. The package is designed to automate the process of distributing and collecting assignments via git repositories hosted on GitHub. As part of this talk will discuss best practices for how these tools can be deployed in a variety of classroom settings from first year introductory courses through graduate level courses. Finally we will discuss how these tools fit into the larger context of modern statistical / data science pedagogy with a particular emphasis on the role of reproducibility in training new researchers.,Education,Cassiopée,Matthias Gehrke
Wed,12:42,Data Science in a Box,Mine Cetinkaya-Rundel,"Data Science in a Box (datasciencebox.org) is an open-source project that aims to equip educators with concrete information on content and infrastructure for designing and painlessly running a semester-long modern introductory data science course with R. In this talk we outline five guiding pedagogical priniples that underlie the choice of topics and concepts introduced in the course as well as their ordering, highlight a sample of examples and assignments that demonstrate how the pedagogy is put into action, introduce `dsbox` -- the companion R package for datasets used in the course, and share sample student work and feedback. We will also walk through a quick start guide for faculty interested in using all or some of these resources in their teaching.",Education,Cassiopée,Matthias Gehrke
Wed,11:30,ClustBlock: a package for clustering datasets,Fabien Llobell,"The clustering of datasets is of paramount interest in multivariate data analysis. In presence of several datasets which pertain to the same individuals but not necessarily the same variables, CLUSTATIS method (Llobell, Cariou, Vigneau, Labenne & Qannari, 2018) operates a cluster analysis of these datasets. This method stands as the core of ClustBlock package. CLUSTATIS strategy consists of a hierarchical algorithm followed by a partitioning algorithm, and yields graphical displays and indices to assess the quality of the solution. A noise cluster option can be activated, with the aim of setting aside atypical datasets. ClustBlock includes specifics functions to perform CLUSTATIS with data from Free Sorting task. An adaptation of CLUSTATIS to the data from a Check All That Apply task, called CLUSCATA (Llobell, Cariou, Vigneau, Labenne & Qannari, 2019), is also available. References Llobell, F., Cariou, V., Vigneau, E., Labenne, A. & Qannari, E. M. (2018). Analysis and clustering of multiblock datasets by means of the STATIS and CLUSTATIS methods. Application to sensometrics. Food Quality and Preference. Llobell, F., Cariou, V., Vigneau, E., Labenne, A., & Qannari, E. M. (2019). A new approach for the analysis of data and the clustering of subjects in a CATA experiment. Food Quality and Preference, 72, 31-39.",Multivariate analysis,Guillaumet 1+2,Éric Matzner-Lober
Wed,11:48,ordinalClust: an R package for analyzing ordinal data.,Margot Selosse,"Ordinal data are a specific kind of categorical data occurring when the levels are ordered. They are used in a lot of domains, specifically when measurements are collected from persons by observations, testings, or questionnaires. ordinalClust is an R package that proposes efficient tools for modeling, clustering, co-clustering, and classification for ordinal data. The data are modeled through the BOS distribution, a gaussian-like distribution parameterized with a position and a precision parameter. On one hand, the co-clustering framework uses the Latent Block Model (LBM) and an SEM-Gibbs algorithm for the parameters inference. On the other hand, the clustering and the classification methods follow on from simplified versions of this algorithm. ",Multivariate analysis,Guillaumet 1+2,Éric Matzner-Lober
Wed,12:06,"funHDDC, a R package to cluster univariate and multivariate functional data",Amandine Schmutz,"The emergence of numerical sensors in many aspects of everyday life leads to the collection of high frequency data. For example in sports, athletes wear devices that collect simultaneously several variables during their training to follow their physical constants. This kind of data can be classified as multivariate functional data. To ease the understanding of those data, there is an increasing need of methods to analyze multivariate functional data. This work presents an R package that provides a clustering technique (Schmutz et al, 2018) that eases the modeling and comprehension of those multivariate functional data. This method, named funHDDC, is based on a functional latent mixture model which allows the partition of data into homogeneous clusters, and an EM algorithm for model inference. In addition to clustering algorithm, the package provides model selection criteria for choosing the number of clusters, and allows the execution of principal component analysis for multivariate functional data. The package usage will be shown on several practical examples whose an original example of horse speed prediction.",Multivariate analysis,Guillaumet 1+2,Éric Matzner-Lober
Wed,12:24,Using the package `simple features` (sf) for sensitivity analysis,Maikol Solís,"The curse of dimensionality is a commonly encountered problem in statistics and data analysis. Variable sensitivity analysis methods are a well studied and established set of tools designed to overcome these sorts of problems. In most cases, those methods require a functional or computer code producing the output variable. Also, they fail to capture relevant features and patterns hidden within the geometry of the enveloping manifold projected onto a variable. In this talk we propose an index that captures, reflects and correlates the relevance of distinct variables within a model by focusing on the geometry of their projections. We first construct the 2-simplices of a Vietoris-Rips complex and then estimate the area of those objects from a data-set cloud. Afterwards, through affine transformations we propose our geometric sensitivity index. As a side result, we could also estimate the regression curve of the data-set generate solely by the estimated manifold. We compile all the analysis in an original package called TopSA (https://github.com/maikol-solis/TopSA), short for Topological Sensitivity Analysis. The package exploits the facility of the package `simple features` (sf) to handle all the geometric operations. The package returns the estimated indexes and has implemented a plot function using `ggplot` together with `geom_sf`.",Multivariate analysis,Guillaumet 1+2,Éric Matzner-Lober
Wed,12:42,Visualizing multivariate linear models in R,Michael Friendly,"This talk reviews our recent work in the development of visualization methods in R for understanding and interpreting multivariate linear models (MLMs).
We begin with a description and examples of the HE plots framework, utilizing the heplots package, wherein multivariate tests can be visualized via ellipsoids in 2D, 3D. HE plots provide visual tests of significance: a term is significant by Roy's test if and only if its H ellipsoid projects somewhere outside the E ellipsoid. 
These graphical methods apply equally to all multivariate designs: MMRA, MANOVA, and MANCOVA, and the test of any linear hypothesis can also be displayed in an HE plot. 
When the rank of the hypothesis matrix for a term is 2 or more, these effects can also be visualized in a reduced-rank canonical space via the candisc package, which also provides new data plots for canonical correlation problems. These plots quite often provide a very simple description of differences in means among groups in a MANOVA setting, by displaying the projections of response variables as vectors in canonical space. We also describe some recent extensions of these ideas:
* We extend the visualization methods to robust MLMs.
* Influence measures and diagnostic plots for MLMs have now been implemented in the mvinfluence package.
* We develop visual tests of equality of covariance matrices for MANOVA.",Multivariate analysis,Guillaumet 1+2,Éric Matzner-Lober
Wed,14:00,Golem : A Framework for Building Robust & Production Ready Shiny Apps,Vincent Guyader,"Shiny has established itself as an essential communication tool in the datascience field, and has gone beyond the scope of simple R users. Its ease of use makes it possible to produce ""quick and dirty"" applications. However, as soon as we try to go beyond the proof-of-concept and produce applications with professional vocation, some development and deployment aspects must be taken into account. Designing a shiny application that is maintainable over time, scalable, tested, robust and deployable is not an easy task. And like everything R-related, there are more wrong ways than right ways to do so.
With the {golem} package, we suggest a development template that relies on a R package. It contains a series of tools that allows you to add functionalities to your application for both development and deployment. For example `golem::add_module()` allows to add a Shiny module, `golem::use_favicon()` allows to add a favicon to the application. In addition, functions such as `golem::add_rconnect_file()` or `golem::gen_dockerfile()` allows you to easily deploy your application on RSconnect or shinyproxy. 
{golem} is an opinionated template designed to help easy development, deployment and maintainance inside a documented R package.",Shiny 2,Concorde 1+2,Colin Fay
Wed,14:18,Art of the Feature Toggle: Patterns for maintaining and improving Shiny applications over time,Kelly Obriant,"Creating one-off shiny applications is easy to do, but what happens when you need to maintain an application over a longer period of time? How do you introduce new application features to an active user-base without disrupting their experience? These can be difficult questions to answer for data scientists who are unfamiliar with the basic principles of software and web application deployments and release cycles. I'll introduce a few core concepts used in the software development world: environment-based and application-based release patterns, and talk about how we can map those techniques to simpler Shiny application maintenance. Finally, I'll demo a Shiny application that leverages `session$user` data to mimic feature toggles for the roll-out of a new feature to a set of users.",Shiny 2,Concorde 1+2,Colin Fay
Wed,14:36,Data for all: Empowering teams with scalable Shiny applications,Alexandra TurcanRuan Pearce-Authers,"Shiny, alongside packages like dplyr and ggplot2, offers an unparalleled developer experience for creating self-service analytics dashboards that empower teams to make data-driven decisions. However, out of the box, Shiny is not well-suited to deployment in a multi-user environment. As part of our mission to establish a data culture in a game development studio, we wanted to deploy a suite of Shiny dashboards such that exploring player behaviour became part of every team's workflow. In this talk, we will discuss the architecture of the supporting cloud infrastructure, including packaging, service orchestration, and authentication. Also, we will show how we've adapted Shiny to a multi-user environment using its new support for promises in combination with the future package. Integrating Shiny into this production-grade architecture allows for a streamlined data science workflow that enables data scientists to focus on creating dashboard content with a built-in code review process, and also to deploy changes to production in a button click. We hope to demonstrate how any data-driven organisation can augment their team-wide workflow by leveraging this end-to-end Shiny pipeline.",Shiny 2,Concorde 1+2,Colin Fay
Wed,14:54,Best practices for building Shiny enterprise applications,Filip Stachura,"Shiny is conquering the enterprise world. Shiny apps are now built for hundreds of users who need reliability and functionality.
Having built and scaled a number of Shiny applications in production, we've learned what's important for a Shiny app to become a stunning success. We'd like to share our experiences with you.
This presentation will give you a deep dive into the best practices for building Shiny production apps, including:
* How to design app architecture for optimal maintainability and development velocity.
* Approaches for avoiding errors in production.
* Test pyramid: why it pays off to have automated end-to-end tests and unit tests, and how to minimize manual testing.
* How to scale Shiny apps to hundreds of users and set up automated performance testing.
* Deployment strategies and the path from deployment, involving many manual steps, to full automation.
Shiny allows you to rapidly iterate on functionality and build impressive dashboards, but it's not easy to do a great job getting that through to production. In this talk, we will discuss what specific steps you can take to achieve reliability without sacrificing speed of development.",Shiny 2,Concorde 1+2,Colin Fay
Wed,14:00,Using AI and R to help improve the quality and health of your personalised food basket.,Peter Jaksons,"At Plant and Food Research we constantly aim to improve to quality and health properties of various fruit and vegetables. Genomic prediction is the science that aims to predict certain fruit and vegetable characteristics based on its DNA. For example, can we use the DNA of an apple to predict if it will taste sweet or has a high vitamin C concentration? Ultimately, can we use the DNA to predict if a consumer might say ""this apple is tasty"" or ""I like the beneficial health effects""? To tackle this daunting problem of linking a plant's DNA to a consumer's spoken word, we need to know everything about the plant, the consumer, and all the steps in between. In this presentation I will discuss the challenges we have to overcome and how R and AI has helped us with genomic prediction by collecting vital information at higher volumes and accuracy that was not possible before. Recent examples are an app we developed that automates sentiment analysis of consumer data, image recognition tools to monitor plants and deep learning methods to evaluate fruit performance in storage rooms.",Applications 2,Cassiopée,Laura Acion
Wed,14:18,Variation of patient turnover on a 30-minutes basis for 3 years: analysis of routine data of a Swiss University Hospital,Sarah N. Musy,"Introduction Hospitals face widely varying care demands intensifying nurses' work, since the number of admissions, discharges and transfers within and between units is fluctuating throughout the day. The objective was to describe patients' movements for a large Swiss University Hospital.    Methods Data from 2015 to 2017 were used containing patients' movements time and dates. With padr package, continuous time was extracted between each movement of the patient for an interval of 30 minutes. The first and last movements were recognized as admission and discharge, respectively. For the transfers, the use of the functions lead() and lag() from dplyr package were used. The goal was to identify for each unit if the patient was coming in or out. Finally, the amount of entries (admissions and transfers in) was plotted against exits (discharges and transfers out) on a pyramid bar chart using ggplot2 and scales packages.   Results Ten departments and 77 units were analyzed with 85,706 patients' observations. The final data comprised of more than 26 million data points. Patient flow varies considerably. While for some units entries and exits occur at the same time, other units have volatile periods with different times of the day where entries and exits occur leading to substantially variation in patient load.   Conclusion It is the first analysis of patient movements conducted on this level of granularity.",Applications 2,Cassiopée,Laura Acion
Wed,14:36,Bridging agent-based modelling and R with nlrx: simulating pedestrian's long-term exposure to air pollution,Hyesop Shin,"Agent-based modelling (ABM) is a bottom-up simulation for simulating actions and interactions between agents. This approach it is particularly useful when modelling human-environment interactions, such as pedestrian's cumulative exposure levels related to travel patterns. Until recently, possible ways to report results were either to bring the text file after simulating the model that is tedious and time-consuming, or to use packages where the codes can only access the ABM platform, which slows the simulation speed (especially for geospatial models). Here, we use a newly released nlrx package that improves the modelling speed, iterates multiple jobs, and provides algorithms for temporal sequence plotting. From our pre-built pedestrian exposure model of Gangnam, Seoul, we demonstrate sequences of pedestrian's health status in every 100 ticks using gganimate, and plot risk population change in geographical hierarchies. Finally, we will discuss our efforts to encourage geographers and modellers to use R as a platform to conduct ABM studies.",Applications 2,Cassiopée,Laura Acion
Wed,14:54,Simulation of the physical movement for Machine Learning with R: Simulation of Robot gait Optimization Using GA,Hae-Yoon Jung,"Recently, machine learning algorithms are used popularly in the engineering field, and simulation of the specific situation becomes one of the important processes. In this talk, we propose an environment for simulation of the physical movement in robots with R. We carried out the optimization of robot gait which is a major issue in the robotics area. As an optimization algorithm, GA (Genetic Algorithm) was used. In our physical simulation, we considered two options. The first option is about position and velocity when forces are applied to the body. We solved this dynamic equation of a robot's body placed in 2-dimensional space using the 3rd order Runge-Kutta method. The second option is for constraining the position and velocity when a body contacts the ground. The sequences of the robot's body position were visualized with animation package in R. This virtual robot has a body, and five parameters determine its walking trajectory. GA package in R was used to optimize these parameters. We successfully get some values that enable robots to walk steady and fast. Through this study, we expect simulation in robotics engineering area can be conducted with R as well.",Applications 2,Cassiopée,Laura Acion
Wed,14:00,Visualisation of open-ended interviews through qualitative coding and cognitive mapping,Frédéric Vanwindekens,"Open-ended interviews are common approaches in social sciences for catching the respondents' worldviews, feeling and knowledge. Their popularity is growing in natural science, particularily for studying social-ecological systems like farms, forests or fisheries. In these systems, decision-making processes and practices are no easily taken into account by classical experimental approaches.  
We developed an original approach that aims to visualize and analyse perceptions, knowledge and practices of managers in social-ecological systems. Based on the usage of R, we contributed to and developed two Shiny Applications that aims to (i) qualitatively code textual documents, like transcribed open-ended interviews ('qcoder') and to (ii) treat the outputs of the qualitative coding in order to draw cognitive maps ('cogmapr'). Cognitive maps are digraphs of variables that can be used as a semi-qualitative model of interviewees' worldviews. The ""cogmapr"" tool contains major functions from our ""Cognitive Mapping Approach for Analysing Actors' Systems of Practices"" : drawing Individual Cognitive Maps, computing Social Cognitive Maps and main indicators from the graph theory.  
The presentation will be supported by case studies from various agroecological systems : grassland, forest in Belgium and Romania, crop diversification in Europe, soil management in Québec.","Social science, marketing & business",Caravelle 2,Giovanna de Vincenzo
Wed,14:18,choicetools: a package for conjoint analysis and best-worst surveys,Chris Chapman,"We present choicetools, a new package to work with data from conjoint analysis and best-worst (aka MaxDiff) stated choice surveys. Users may supply their own choice data, or import data from two popular survey platforms, Qualtrics (best-worst surveys) and Sawtooth Software Lighthouse Studio (conjoint and best-worst). choicetools estimates classical and hierarchical Bayesian models, performs preference share market simulation, and plots aggregate and individual-level estimates. It is also useful to teach conjoint analysis; it provides functions to create and field a survey in CSV format to be completed in a spreadsheet application in a classroom setting, and to estimate preferences from the responses. In our UseR! talk, we demonstrate a vignette style choice-based conjoint project using simulated responses, including survey specification, spreadsheet-style presentation, estimation, and plotting. We briefly discuss experimental features for attribute importance, and answer audience questions. This talk will be of interest to research practitioners in marketing, transportation, and other fields, and to academics who teach discrete choice and conjoint analysis methods.","Social science, marketing & business",Caravelle 2,Giovanna de Vincenzo
Wed,14:36,Robust mediation analysis using the R package robmed,Andreas Alfons,"Mediation analysis is one of the most widely used statistical techniques in the social and behavioral sciences. In its simplest form, a mediation model allows to study how an independent variable (X) affects a dependent variable (Y) through an intervening variable that is called a mediator (M). Such an analysis is often carried out via a pair of regression models, in which case the indirect effect of X on Y through M can be computed as a product of coefficients from those regression models. The standard test for this indirect effect is a bootstrap test based on ordinary least squares (OLS) regressions. However, this test is very sensitive, e.g., to outliers or heavy tails, which poses a serious threat to empirical testing of theory about mediation mechanisms. The R package robmed implements a robust test for mediation analysis based on the fast and robust bootstrap methodology for robust regression estimators. This procedure yields reliable results for estimating the effect size and assessing its significance, even when the data deviate from the usual normality assumptions. In addition to simple mediation models, the package also provides functionality for mediation models with multiple mediators as well as control variables. Furthermore, the standard bootstrap test and other proposals are included in the package. We will demonstrate the use of package robmed in an example from management research.","Social science, marketing & business",Caravelle 2,Giovanna de Vincenzo
Wed,14:54,propro: Enhancing Discovered Process Models using Bayesian Inference and MCMC,Gert Janssenswillen,"Process mining is an innovative research field aimed at extracting useful information about business processes from event data. An important task herein is process discovery; the discovery of process models from data. The results of process discovery are mainly deterministic process models, which do not convey a notion of probability or uncertainty. In this paper, Bayesian inference and Markov Chain Monte Carlo is used to build a statistical model on top of a process model using event data, which is able to generate probability distributions for choices in a process' control-flow. A generic algorithm to build such a model is presented, and it is shown how the resulting statistical model can be used to test different kinds of hypotheses, such as non-deterministic dependencies between different choices in the model. The algorithm is implemented in a new R package, named propro, for probabilistic process models. In this paper, it is shown how propro can be used in real-life process analysis scenarios and leads to valuable information about the processes under consideration, which go beyond the discovery of static control-flow. As a result, propro supports the enhancement of discovered process models by exposing probabilistic dependencies, and allows to compare the quality among different models, each of which provides important advancements in the field of process mining.","Social science, marketing & business",Caravelle 2,Giovanna de Vincenzo
Wed,14:00,Navigating through the R packages for movement,Rocio Joo,"The advent of biologging devices has led to the collection of ever-increasing quantities of tracking data from animals and humans. In parallel, sophisticated tools to process, visualize and analyze tracking data have been developed in abundance. Within the R software, we listed 57 packages focused on these tasks, called here tracking packages. Here we review and describe each package based on a workflow centered around tracking data, broken down in three stages: pre-processing, post-processing and analysis (data visualization, track description, path reconstruction, behavioral pattern identification, space use characterization, trajectory simulation and others). Links between packages are assessed through a network graph analysis and show that one third of the packages worked on isolation, reflecting a fragmentation in the R movement-ecology programming community. Finally, we provide recommendations for users to choose packages and for developers to maximize the usefulness of their contribution and strengthen the links between the programming community.",Movement & transport,Ariane 1+2,Angela Li
Wed,14:18,"Classes, methods and data analysis for trajectories",Mohammad Mehdi Moradi,"Object tracking has been recently considered drastically to get insight into the behavior of moving objects within different contexts such as eye tracking, animal tracking, traffic analysis, city management, sports, etc. This, however, results in a huge amount of irregular data which are usually not well structured, depending on the tracking methods and the aim of the study. In this work, we review different classes and methods from R package trajectories with the aim of structuring, handling and summarizing movement data. Simulation techniques and model fitting are also studied. We further proceed with some statistical characteristics based on spatial point processes such as first- and second-order summary statistics to analyze the behavior of objects over time in terms of distribution and pairwise interaction. We finally demonstrate our findings of a taxi movement data from Beijing, China.",Movement & transport,Ariane 1+2,Angela Li
Wed,14:36,Modelling spatial flows with R,Christine Thomas-Agnan,"We present an R implementation of spatial interaction models. Flow data represent movements of people or goods between two spatial locations, for example in migration, international trade, transportation. Gravity models, which have been extensively used for this purpose, include a function of distance between origin and destination among the explanatory variables to account for the spatial dimension. To further eliminate the spatial structure present in this type of data, we implement two fitting methods for spatial autoregressive models accounting for spatial dependence between flows: a Bayesian approach and a three stage least squares approach. We discuss impacts measures evaluation as well and extend these computations to the case of different characteristics at origin and destination. Contrary to existing programs, our implementation with the R language also allows for a different list for origins and destinations. We illustrate with an application to air transportation data.   LeSage, J. P., and Thomas-Agnan, C. (2015). Interpreting spatial econometricorigin-destination flow models. Journal of Regional Science, 55(2),188-208. Margaretic, P., Thomas-Agnan, C., & Doucet, R. (2017). Spatial dependence in (origin-destination) air passenger flows. Papers in Regional Science, 96(2), 357-380.",Movement & transport,Ariane 1+2,Angela Li
Wed,14:54,R for Transport Planning,Robin Lovelace,"Since the first release of R on CRAN, in 1997, its use in many fields has grown rapidly. Lai et al. (2019), for example, suggest that more than 50% of research articles published in Ecology use R in some way. Much like many ecological datasets, transport data tend to be large, diverse and have spatial and temporal attributes. Unlike Ecology, Transport Planning has been a slow adopter of R, with a much lower percentage of papers using the language. This raises the question: why? After exploring this question, in relation to dominant transport planning software products, the talk will sketch of what an open source transport planning 'ecosystem' could look like. Based on my own experience, of developing the stplanr package and teaching practitioners, the talk will discuss the importance of building ""communities of practice"", for transport planners making the switch to R. These observations relate to others promoting R in new environments, and link to the wider question of how to advocate for open source software in wider society. Lai, J., Lortie, C.J., Muenchen, R.A., Yang, J., Ma, K., 2019. Evaluating the popularity of R in ecology. Ecosphere 10.",Movement & transport,Ariane 1+2,Angela Li
Wed,14:00,Package flextable: a grammar to produce tabular reporting from R,Quentin Fazilleau,"The flextable package provides an interface to generate tables for publication and corporate reporting. Originally designed to work with the package officer, it has evolved this year to get compatible with the R Markdown format.
The package enables simple and complex tabular reporting composition thanks to a user-friendly grammar. It supports output to HTML, Word, PowerPoint and recently PDF through the pagedown package.
In this talk I will introduce the main concepts of the package and demonstrate them with simples examples. I will show how to manage the layouts, how to format the content and mix images with text. Finally, I will expose a concrete implementation inside an R Markdown report and a Shiny application.",Reproducibility,Saint-Exupéry,Emma Rand
Wed,14:18,Connecting R/R Markdown and Microsoft Word using StatTag for Collaborative Reproducibility,Leah Welty,"Although R Markdown can render documents to Microsoft Word, R/R Markdown users must sometimes transcribe statistical content in to separate Microsoft Word documents (e.g., documents drafted by colleagues in Word, or documents that must be prepared in Word), a process that is error prone, irreproducible, and inefficient. We will present StatTag (www.stattag.org), an open source, free, and user-friendly program we developed to address this problem. StatTag establishes a bi-directional link between R/R Markdown files and a Word document, and supports a reproducible pipeline even when: (1) statistical results must be included and updated in Word documents that were never generated from Markdown;  and (2) text in Word files generated from R/R Markdown has departed substantially from original Markdown content, for example through tracked changes or comments. We will demonstrate how to use StatTag to connect R/R Markdown and Word files so that all files can be edited separately, but statistical content – values, tables, figures, and verbatim output – can be updated automatically in Word. Using practical examples, we will also illustrate how to use StatTag to view, edit, and rerun R/R Markdown code directly from Word. ",Reproducibility,Saint-Exupéry,Emma Rand
Wed,14:36,"The ""Rmd first"" method: when projects start with the documentation",Sébastien Rochette,"Analysis, R programs or packages are easier to use when correctly documented. However, documentation is perceived time consuming and is the poor child of development projects. We assume that any kind of R project (data analysis, R package, shiny applications) can embrace the literate programming paradigm: explanation of the program logic in natural langage interlaced with code snippets. Hence, Rmarkdown files are ideal candidates for reproducible projects and workflows. The ""Rmd first"" approach proposes a project in four parts: (1) Prototype: keep track of data explorations, graphs and tables using Rmd files. (2) Package: regularly transform code chunks into functions to simplify and clarify reading. Extracted functions are documented and tested. The Rmd file ends as the vignette of the package. (3) Modularize: separate projects into multiple sub-parts, hence Rmd files. It simplifies participation of multiple developers by reducing potential files conflicts. (4) Deploy: use available tools to share your work, confidentially or publicly. In a vignette documented package, automated report may be built with {bookdown}, a website may present the full documentation with {pkgdown}. This approach allows developers to explain their process during development. At low cost, documentation is already available, the analysis is directly useable and reproducible, the report already written.",Reproducibility,Saint-Exupéry,Emma Rand
Wed,14:54,R gnumaker: easy Makefile construction for enhancing reproducible research,Peter Baker,"Is there a crisis in reproducible research? Some studies such as Ioannidis et. el (2009) have estimated that over fifty percent of published papers in some fields of research are not reproducible. In statistical and data analysis projects, this appears to be due to lack of training and poor tools rather than scientific fraud.  
In the R community, there is a move towards Don't Repeat Yourself (DRY) approaches to reproducible research and reporting. Employing computing tools like GNU Make to regenerate output when syntax or data files change, GNU git for version control, writing GNU R functions or packages for repetitive tasks and R Markdown for reporting can greatly improve reproducibility.  
The gnumaker package is ideal for statisticians and data analysts with little experience of Make. By specifying the relationships between syntax and data files gnumaker makes it easy to create and plot GNU Makefiles as DAGs. Gnumaker employs Make pattern rules for R, Sweave, R Markdown, Stata, SAS, etc outlined in P Baker (2019) Using GNU Make to Manage the Workflow of Data Analysis Projects, Journal of Statistical Software (Accepted). See https://github.com/petebaker",Reproducibility,Saint-Exupéry,Emma Rand
Wed,14:00,Multi-data learning with M-ABC: extending your ABC's,Marijke Van Moerbeke,"The paradigm of the current revolution in biomedical studies, and life-sciences in general, is to focus on a better understanding of biological processes. In the early stages of drug development, different types of information on the compounds are collected: the chemical structures of the molecules, the predicted targets (target predictions), various bioassays, the toxicity and more. An analysis of each data set could reveal interesting yet disjoint information. An important task is the integration of the data sets from the experiments to understand the working mechanism of the drug. Multi-source clustering methods aim to discover groups of objects that are consistently similar across data sets. We introduce a new multi-source clustering method, M-ABC, which applies a consensus function generating a clustering of clusters. This can improve the quality of individual clustering algorithms. The proposed method is implemented and publicly available in the R package IntClust which is a wrapper package for a multitude of ensemble clustering methods. In addition, visualization for a comparison of clustering results and cluster specific secondary analyses such as differential gene expression and pathway analysis are available.",Bioinformatics 1,Guillaumet 1+2,Mélina Gallopin
Wed,14:18,Fast and Optimal Peak Detection in Large Genomic Data via the PeakSegDisk Package,Toby Hocking,"We describe a new algorithm and R package for peak detection in genomic data sets using constrained optimal changepoint algorithms. These detect changes from background to peak regions by imposing the constraint that the mean should alternately increase then decrease. An existing algorithm for this problem exists, and gives state-of-the-art accuracy results, but it is computationally expensive when the number of changes is large. We propose an algorithm with empirical O(N log N) time complexity that jointly estimates the number of peaks and their locations by globally minimizing a non-convex penalized cost function. We also propose a sequential search algorithm that finds the best solution with K segments in O(N log(N) log(K)) time, which is much faster than the previous O(K N log N) algorithm. Our empirical results show that our disk-based implementation in the PeakSegDisk R package can be used to quickly compute constrained optimal models with many changepoints, which are needed to analyze typical genomic data sets that have tens of millions of observations. ",Bioinformatics 1,Guillaumet 1+2,Mélina Gallopin
Wed,14:36,PEREpigenomics: a shiny app to visualize Roadmap Epigenomics data,Guillaume Devailly,"The Roadmap Epigenomics consortium has gathered and produced abundant epigenomic data in human cell lines and tissues to build reference epigenetic maps. The data is freely available, and is exploitable through web browsers. It has been used by the consortium to classify chromatin into ""states"" that can be more easily compared across cell type. Here we systematically assessed the links between epigenetic marks and transcription by generating ""stacked profiles"" of epigenetic marks at transcription start sites (TSS), transcription end sites (TES) and middle exons, sorted according to expression levels or exon inclusion ratio, for each cell type and tissue in the dataset. The thousands of attractive visualisations generated are made easily browsable through a web application, www.perepigenomics.roslin.ed.ac.uk. We also investigated how change of epigenetic marks across cells were correlated to change of expression level / inclusion ratio, allowing us to build a comprehensive understanding of the associations between epigenetic marks at TSS, TES and middle exons and expression levels or exon inclusion ratio.",Bioinformatics 1,Guillaumet 1+2,Mélina Gallopin
Wed,14:54,clustDRM: an R package and Shiny app for modeling high- throughput dose-response data,Vahid Nassiri,"Dose-response modeling is a crucial step in drug discovery and safety assessment. R packages like ""drc"" and ""DoseFinding"" provide useful tools to fit dose-response models and estimate parameters such as effective doses. When it comes to modeling dose-response patterns of several compounds (e.g., in high content screening studies), one may face with three different challenges:  1. the dose-response relationship may be non-existent (flat patterns),  2. no single dose-response model fits the data well enough,  3. due to the use of often complex non-linear models the estimation could become computationally intensive (especially in a high-throughput setting).  To address these issues, we have developed an R package called 'clustDRM` that provides a unified platform to analyze dose-response relationships in a high-throughput setting. The package uses a two-stage approach. First, it filters out the compounds with a flat dose-response pattern and identifies the patterns of the non-flat ones. Next, it fits a set of appropriate dose-response models (accounting for the previously identified patterns) and uses model-averaging to estimate effective doses and their standard errors. Parallel computations are used to address the third issue. Furthermore, a Shiny app accompanies the package to make its use easier for scientists from various fields without deep knowledge of R.",Bioinformatics 1,Guillaumet 1+2,Mélina Gallopin
Wed,16:00,A missing value tour in R,Julie Josse,"In many application settings, the data have missing features which make data analysis challenging. An abundant literature addresses missing data as well as more than 150 R packages. Funded by the R consortium, we have created the R-miss-tastic plateform along with a dedicated task view which aims at giving an overview of main references, contributors, tutorials to offer users keys to analyse their data. This plateform highlights that this is an active field of work and that as usual different problems requires designing dedicated methods.
In this presentation, I will share my experience on the topic. I will start by the inferential framework, where the aim is to estimate at best the parameters and their variance in the presence of missing data. Last multiple imputation methods have focused on taking into account the heterogeneity of the data (multi-sources with variables of different natures, etc.). Then I will present recent results in a supervised-learning setting. A striking one is that the widely-used method of imputing with the mean prior to learning can be consistent. That such a simple approach can be relevant may have important consequences in practice.",Keynote,Concorde 1+2,Diane Beldame
